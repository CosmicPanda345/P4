{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **一、Langchain概述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Langchain概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain提供了一套工具、组件和接口，可简化创建由大型语言模型 (LLM) 和聊天模型提供支持的应用程序的过程。\n",
    "\n",
    "LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源。\n",
    "\n",
    "LangChain是一个基于语言模型开发应用程序的框架。\n",
    "\n",
    "- 数据感知：将语言模型连接到其他数据源\n",
    "- 自主性：允许语言模型与其环境进行交互\n",
    "\n",
    "主要价值在于：\n",
    "\n",
    "- 组件化：为使用语言模型提供抽象层，以及每个抽象层的一组实现。组件是模块化且易于使用的，无论您是否使用LangChain框架的其余部分。\n",
    "- 现成的链：结构化的组件集合，用于完成特定的高级任务\n",
    "\n",
    "LangChain 为特定用例提供了多种组件，例如个人助理、文档问答、聊天机器人、查询表格数据、与 API 交互、提取、评估和汇总。\n",
    "\n",
    "LangChain 中的模型分类：\n",
    "\n",
    "- LLM（大型语言模型）：这些模型将文本字符串作为输入并返回文本字符串作为输出。它们是许多语言模型应用程序的支柱。\n",
    "\n",
    "- 聊天模型( Chat Model)：聊天模型由语言模型支持，但具有更结构化的 API。\n",
    "\n",
    "  ​\t他们将聊天消息列表作为输入并返回聊天消息。\n",
    "\n",
    "  ​\t这使得管理对话历史记录和维护上下文变得容易。\n",
    "\n",
    "- 文本嵌入模型(Text Embedding Models)：这些模型将文本作为输入并返回表示文本嵌入的浮点列表。\n",
    "\n",
    "  这些嵌入可用于文档检索、聚类和相似性比较等任务。\n",
    "\n",
    "LangChain 的特点：\n",
    "\n",
    "- LLM 和提示：LangChain 使管理提示、优化它们以及为所有 LLM 创建通用界面变得容易。此外，它还包括一些用于处理 LLM 的便捷实用程序。\n",
    "- 链(Chain)：这些是对 LLM 或其他实用程序的调用序列。LangChain 为链提供标准接口，与各种工具集成，为流行应用提供端到端的链。\n",
    "\n",
    "- 数据增强生成：LangChain 使链能够与外部数据源交互以收集生成步骤的数据。例如，它可以帮助总结长文本或使用特定数据源回答问题。\n",
    "- Agents：Agents 让 LLM 做出有关行动的决定，采取这些行动，检查结果，并继续前进直到工作完成。LangChain 提供了代理的标准接口，多种代理可供选择，以及端到端的代理示例。\n",
    "\n",
    "- 内存：LangChain 有一个标准的内存接口，有助于维护链或代理调用之间的状态。它还提供了一系列内存实现和使用内存的链或代理的示例。\n",
    "- 评估：很难用传统指标评估生成模型。这就是为什么 LangChain 提供提示和链来帮助开发者自己使用 LLM 评估他们的模型。\n",
    "\n",
    "\n",
    "\n",
    "LangChain提供了以下主要组件：\n",
    "\n",
    "\\- 模型 ( Models )\n",
    "\n",
    "\\- 提示词 ( Prompt )\n",
    "\n",
    "\\- 代理（ Agents ）\n",
    "\n",
    "\\- 链（ Chains ）\n",
    "\n",
    "\\- 索引 ( Indexes )\n",
    "\n",
    "\\- 内存（Memory）\n",
    "\n",
    "\\- 模式 （ Schema ）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2应用场景\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **信息检索**\n",
    "   - LangChain可以根据用户的查询意图，通过记忆链中的语义信息，提供准确、全面的搜索结果。\n",
    "   - 无论是文本、图片还是视频等多媒体数据，LangChain都能进行深度理解和检索。\n",
    "2. **问答系统**\n",
    "   - LangChain可以根据用户的问题，从记忆链中抽取相关信息，并给出准确的答案。\n",
    "   - 无论是常见问题还是专业领域的知识，LangChain都能提供高质量的回答。\n",
    "3. 个性化推荐\n",
    "   - LangChain可以根据用户的兴趣和偏好，从记忆链中推荐相关的内容。\n",
    "   - 无论是新闻、音乐还是电影等，LangChain都能根据用户的历史行为和喜好进行精准推荐。\n",
    "4. 机器翻译\n",
    "   - LangChain可以利用记忆链中的语义信息，进行更加准确、自然的机器翻译。\n",
    "   - 通过对源语言和目标语言的语义关联进行建模，LangChain可以提供更加流畅、准确的翻译结果。\n",
    "5. 聊天机器人\n",
    "   - LangChain可以用于构建聊天机器人，使其具备更丰富的交互能力和更准确的回答能力。\n",
    "   - 通过与大型语言模型的结合，聊天机器人可以更加智能地理解用户意图，并提供相应的回复。\n",
    "6. **生成式问答（GQA）和摘要**\n",
    "   - LangChain允许将语言模型与其他数据源连接在一起，实现数据感知，从而支持生成式问答和摘要等任务。\n",
    "   - 这些任务通常需要处理大量的文本数据，LangChain的模块化设计和对LLM的通用接口实现可以极大地简化开发过程。\n",
    "7. 结合大型语言模型、知识库和计算逻辑快速开发AI应用\n",
    "   - LangChain通过其强大的框架和灵活的表达语言（LCEL），支持并行化、回退、批处理、流式传输和异步操作等功能。\n",
    "   - 这使得开发人员能够结合大型语言模型、知识库和计算逻辑快速开发强大的AI应用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装\n",
    "~~~\n",
    "pip install langchain==0.1.6\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、导入大模型的类\n",
    "from langchain_community.llms import Tongyi\n",
    "# 实例化\n",
    "llm = Tongyi()\n",
    "# 调用通义千问\n",
    "ret =  llm.invoke(\"你是谁？\")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1Prompt介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 中的 \"prompt\" 是一个关键概念，它指的是输入给大型语言模型（LLM）的文本指令或提示，用于引导模型生成特定的输出或执行特定的任务。在 LangChain 的框架中，prompt 的设计和使用对于构建高效、准确的链式应用至关重要。\n",
    "\n",
    "以下是一些 LangChain 中 prompt 的应用场景和重要性：\n",
    "\n",
    "1. **任务定义**：通过精心设计的 prompt，可以明确告诉 LLM 要执行什么任务。例如，对于问答系统，prompt 可能包含问题文本和指示模型生成答案的指令。\n",
    "2. **上下文提供**：Prompt 可以包含必要的上下文信息，以帮助 LLM 理解当前任务的背景和上下文。这对于处理具有复杂依赖关系或需要跨多个步骤推理的任务尤为重要。\n",
    "3. **示例引导**：通过提供示例 prompt（即少样本学习或零次学习中的例子），可以指导 LLM 如何生成符合要求的输出。这种方法特别适用于那些难以用明确规则定义的任务。\n",
    "4. **链式推理**：在 LangChain 中，prompt 可以用于构建链式推理流程。通过设计一系列相互关联的 prompt，可以引导 LLM 逐步完成复杂的推理任务，如多步骤问题解答或对话生成。\n",
    "5. **安全和合规性**：通过适当的 prompt 设计，可以确保 LLM 的输出符合特定的安全和合规性要求。例如，可以通过在 prompt 中包含适当的过滤器或指导原则来避免生成不适当或冒犯性的内容。\n",
    "6. **性能优化**：Prompt 的设计也可以影响 LLM 的性能和效率。通过优化 prompt 的长度、结构和内容，可以提高 LLM 的响应速度和输出质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2案例实现流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "实现流程：\n",
    "\n",
    "1.导入prompt的类\n",
    "\n",
    "2.导入通义大模型\n",
    "\n",
    "3.定义一个模板\n",
    "\n",
    "4.实例化模板类\n",
    "\n",
    "5.提醒用户输入\n",
    "\n",
    "6.生成prompt\n",
    "\n",
    "7.实例化通义大模型\n",
    "\n",
    "8.调用invoke问\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1导入prompt的类\n",
    "from langchain.prompts import PromptTemplate\n",
    "# 导入通义大模型\n",
    "from langchain_community.llms import Tongyi\n",
    "# 定义一个模板\n",
    "pp = \"{county}的首都是哪里？\"\n",
    "# 实例化模板类\n",
    "promptTemplate = PromptTemplate.from_template(pp)\n",
    "# 输入\n",
    "ins = input(\"请输入国家名：\")\n",
    "# 生成prompt\n",
    "prompt = promptTemplate.format(county=ins)\n",
    "print(prompt)\n",
    "# 实例化通义大模型\n",
    "tongyi = Tongyi()\n",
    "ret = tongyi.invoke(prompt)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**格式化输出**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "#自定义class，继承了BaseOutputParser\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ChatPromptTemplate是一种用于帮助人们更好地进行对话和交流的工具，特别是在现代社会中，随着信息爆炸和人们日益依赖文字和数字进行沟通的背景下，其重要性愈发凸显。以下是关于ChatPromptTemplate的详细解释：\n",
    "\n",
    "1. 概述\n",
    "   - ChatPromptTemplate是一个模板化的对话工具，它允许用户创建、设定对话内容和格式，并与他人进行分享和互动。\n",
    "   - 通过使用ChatPromptTemplate，用户能够以一种简洁、清晰的方式组织和展示对话内容，从而提高沟通效率，减少信息混乱和误解的可能性。\n",
    "2. 功能\n",
    "   - **上下文理解**：ChatPromptTemplate的上下文功能有助于用户更好地理解对话的背景和情境，从而更好地参与和回应对话内容。\n",
    "   - **模板创建**：用户可以轻松创建对话模板，这些模板可以包含各种元素，如文字、图片、信息等，以适应不同的沟通需求。\n",
    "   - **角色和参数**：在ChatPromptTemplate中，聊天消息可以与内容和一个称为角色的额外参数相关联。例如，在OpenAI Chat Completions API中，聊天消息可以与AI助手、人类或系统角色相关联。\n",
    "3. 使用示例\n",
    "   - ChatPromptTemplate的使用可以基于不同的库和框架，如LangChain等。通过调用相关的类和函数，用户可以构建和处理提示模板。\n",
    "   - 示例代码可能包括从特定库中导入ChatPromptTemplate，并通过格式化或提供参数来生成具体的聊天提示。\n",
    "4. 应用场景\n",
    "   - ChatPromptTemplate广泛应用于各种聊天软件和社交平台，特别是在需要高效、清晰沟通的场景中，如客服系统、在线教育、社交媒体等。\n",
    "5. 优势\n",
    "   - 提高沟通效率：通过预定义的模板和格式，用户可以更快地创建和发送对话内容。\n",
    "   - 减少误解：清晰的模板和上下文功能有助于减少信息混乱和误解的可能性。\n",
    "   - 可定制性：ChatPromptTemplate允许用户根据自己的需求创建和修改模板，以满足不同的沟通场景。\n",
    "\n",
    "**区别**\n",
    "\n",
    "- PromptTemplate：这是一种通用形式的模板，用于生成系统提示或信息模板。它可以应用于多种场景，如用户输入错误时的提示、系统操作成功时的提示、数据加载提示、信息确认提示以及活动推广提示等。它并不特定于对话或聊天场景。\n",
    "- ChatPromptTemplate：这是PromptTemplate在聊天领域的一个特殊表达。它专注于对话和聊天场景，帮助用户更好地组织和展示对话内容。ChatPromptTemplate的上下文功能使得用户能够更好地理解对话的背景和情境，从而更好地参与和回应对话内容。\n",
    "\n",
    "\n",
    "\n",
    "聊天模型是语言模型的一个变体，聊天模型以语言模型为基础，其内部使用语言模型，不再以文本字符串为输入和输出，而是将聊天信息列表为输入和输出，他们提供更加结构化的 API。通过聊天模型可以传递一个或多个消息。LangChain 目前支持四类消息类型：分别是 AIMessage、HumanMessage、SystemMessage 和 ChatMessage 。\n",
    "\n",
    "- SystemMessage：系统消息是用来设定模型的一种工具，可以用于指定模型具体所处的环境和背景，如角色扮演等；\n",
    "- HumanMessage：人类消息就是用户信息，由人给出的信息，如提问；使用 Chat Model 模型就得把系统消息和人类消息放在一个列表里，然后作为 Chat Model 模型的输入\n",
    "- AIMessage：就是 AI 输出的消息，可以是针对问题的回答\n",
    "- ChatMessage：Chat 消息可以接受任意角色的参数\n",
    "\n",
    "大多数情况下，我们只需要处理 HumanMessage、AIMessage 和 SystemMessage 消息类型。此外聊天模型支持多个消息作为输入，如下系统消息和用户消息的示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 ChatPromptTemplate案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入通义大模型\n",
    "from langchain_community.llms import Tongyi\n",
    "# 导入模板类\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 定义模板\n",
    "message = [   \n",
    "        (\"system\",\"假设你是起名字的大师，\"),\n",
    "        (\"human\", \"我家是{sex}宝，姓{firstName}，请起3个好养的名字？\"),\n",
    "]\n",
    "# 实例化模板类\n",
    "chartmp =  ChatPromptTemplate.from_messages(message);\n",
    "prompt = chartmp.format_messages(sex=\"男\",firstName=\"李\")\n",
    "print(prompt)\n",
    "# 实例化通义大模型\n",
    "tongyi = Tongyi()\n",
    "ret = llm.invoke(prompt)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ChatMessagePromptTemplate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatMessagePromptTemplate是LangChain框架中用于生成特定角色或自定义角色对话提示的一个模板。这个模板允许用户指定角色的名称，并基于提供的模板字符串生成相应的聊天消息提示。以下是关于ChatMessagePromptTemplate的详细解释：\n",
    "\n",
    "1. **作用**：\n",
    "   - ChatMessagePromptTemplate主要用于在对话模型（chat model）中，封装并生成具有特定角色或自定义角色的聊天消息提示。\n",
    "2. **使用方式**：\n",
    "   - 用户首先需要从`langchain.prompts`模块中导入ChatMessagePromptTemplate类。\n",
    "   - 然后，用户可以定义一个包含占位符（如`{subject}`）的模板字符串。\n",
    "   - 使用`ChatMessagePromptTemplate.from_template()`方法，用户可以指定角色的名称和模板字符串来创建一个ChatMessagePromptTemplate对象。\n",
    "   - 接着，用户可以通过调用该对象的`format()`方法，并传入相应的参数来替换模板字符串中的占位符，从而生成具体的聊天消息提示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "message = \"请帮我写一篇关于{type}的文章\"\n",
    "# 实例化\n",
    "promptTemplate = ChatMessagePromptTemplate.from_template(role = \"全科医生\",template=message)\n",
    "prompt = promptTemplate.format(type=\"心脏病\")\n",
    "print(prompt.content)\n",
    "# 实例化通义大模型\n",
    "tongyi = Tongyi()\n",
    "ret = tongyi.invoke(prompt.content)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 **StringPromptTemplate**自定义模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在LangChain中，`StringPromptTemplate`（尽管在提供的参考文章中并未直接提及这个名字，但我们可以基于上下文和一般性的理解来讨论）可能是一个用于生成字符串形式Prompt的模板类。以下是对`StringPromptTemplate`（或类似概念）在LangChain中的可能用途和特性的描述：\n",
    "\n",
    "1. 定义与用途\n",
    "\n",
    "- **定义**：`StringPromptTemplate`是一个类，它允许用户通过提供模板字符串和参数来生成自定义的Prompt。\n",
    "- **用途**：在LangChain中，Prompt是引导大语言模型（LLM）生成响应的关键输入。`StringPromptTemplate`提供了一种灵活且可重复的方式来创建这些Prompt。\n",
    "\n",
    "2. 特性\n",
    "\n",
    "- **参数化**：模板字符串可以包含占位符，这些占位符在生成Prompt时会被实际参数替换。\n",
    "- **灵活性**：用户可以根据需要自定义模板字符串，以生成适应不同任务和场景的Prompt。\n",
    "- **易于使用**：通过简单的API调用，用户可以快速地根据模板和参数生成所需的Prompt。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入自定义的Prompt模板\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "# 代码解析器\n",
    "import inspect\n",
    "\n",
    "Prompt = \"假设你是一个非常擅长编程的AI，现在给你如下函数名称，你会按照如下格式，输出这段代码的名称、源代码、中文解释。\\n函数名称: {function_name}\\n返回类型：{function_type}\\n源代码:\\n{source_code}\\n代码解释:\\n\"\n",
    "# 自定义的函数\n",
    "def HelloWorld(abc):\n",
    "    print(\"Hello World\"+ abc)\n",
    "\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # 获得源代码\n",
    "        code = inspect.getsource(kwargs[\"function_name\"])\n",
    "        # 生成提示词模板\n",
    "        prompt = Prompt.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, function_type = kwargs[\"function_type\"] , source_code=code\n",
    "        )\n",
    "        return prompt\n",
    "  \n",
    "pp = CustmPrompt(input_variables=[\"function_name\",\"function_type\"])\n",
    "prop = pp.format(function_name=HelloWorld,function_type=\"int\")\n",
    "tongyi = Tongyi()\n",
    "ret = tongyi.invoke(prop)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5**使用jinja2与f-string来实现提示词模板格式化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1安装\n",
    "pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.2模板格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "pp = \"{{county}}的首都是哪里？\"\n",
    "# 实例化模板类\n",
    "promptTemplate = PromptTemplate.from_template(pp,template_format=\"jinja2\")\n",
    "promt1 = promptTemplate.format(county=\"中国\")\n",
    "print(promt1)\n",
    "\n",
    "pp1 = \"{county}的首都是哪里？\"\n",
    "# 实例化模板类\n",
    "promptTemplate = PromptTemplate.from_template(pp1,template_format=\"f-string\")\n",
    "promt = promptTemplate.format(county=\"中国\")\n",
    "print(promt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.6.使用PipelinePromptTemplate实现多步提示词**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1多步提示词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\- Final prompt: 最终返回的提示词模板\n",
    "\n",
    "\\- Pipeline prompts：组成提示词管道的模板\n",
    "\n",
    "\\```\n",
    "\n",
    "\\#角色\n",
    "\n",
    "你是一个擅长成语接龙的游戏玩家。\n",
    "\n",
    "\\## 技能\n",
    "\n",
    "\\### 技能1:进行成语接龙游戏\n",
    "\n",
    "1.当对方说出一个成语后，你首先判断它是不是一个成语。如果不是则提示用户，要求用户重新输入;如果是成语，则需要根据该成语的最后一个字，说出一个新的成语。\n",
    "\n",
    "2.可以使用同音字接龙。\n",
    "\n",
    "3.你说的必须是“成语”，不能是一般的词语\n",
    "\n",
    "4.当用户或你无法找到下一个成语时，此游戏结束。用户可以输入一个新的成\n",
    "\n",
    "语，重新开始接龙\n",
    "\n",
    "\\## 限制\n",
    "\n",
    "只进行成语接龙游戏，拒绝回答其他话题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 管道模板\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "# Final Prompt由一系列变量构成\n",
    "full_template = \"{name}{skill}{limit}\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "# 第一层基本设计\n",
    "name = \"\"\"#角色\n",
    "你是一个擅长{name}的游戏玩家。\"\"\"\n",
    "name_prompt = PromptTemplate.from_template(name)\n",
    "\n",
    "# 第二基本设计\n",
    "skill = \"\"\"## 技能\n",
    "{skill}\"\"\"\n",
    "skill_prompt = PromptTemplate.from_template(skill)\n",
    "\n",
    "# 第三基本设计\n",
    "limit = \"\"\"## 限制\n",
    "{limit}\"\"\"\n",
    "limit_prompt = PromptTemplate.from_template(limit)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"name\", name_prompt),\n",
    "    (\"skill\", skill_prompt),\n",
    "    (\"limit\", limit_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n",
    "\n",
    "ret1 = pipeline_prompt.format(\n",
    "    name=\"石头剪刀布\", \n",
    "    skill=\"\"\"### 技能1:进行石头剪刀布游戏\n",
    "1.当对方说出一个石头剪刀布后，你首先判断它是不是一个石头剪刀布。如果不是则提示用户，要求用户重新输入;如果是剪刀，则需要根据该剪刀的最后一个字，说出一个新的剪刀。\n",
    "2.你可以使用石头剪刀布接龙。\n",
    "3.你说的必须是“剪刀”，不能是一般的词语\"\"\", \n",
    "limit=\"只进行石头剪刀布游戏，拒绝回答其他话题\")\n",
    "print(ret1)\n",
    "\n",
    "\n",
    "ret2 = pipeline_prompt.format(\n",
    "    name=\"成语接龙\", \n",
    "    skill=\"\"\"### 技能1:进行成语接龙游戏\n",
    "1.当对方说出一个成语后，你首先判断它是不是一个成语。如果不是则提示用户，要求用户重新输入;如果是成语，则需要根据该成语的最后一个字，说出一个新的成语。\n",
    "2.可以使用同音字接龙。\n",
    "3.你说的必须是“成语”，不能是一般的词语\n",
    "4.当用户或你无法找到下一个成语时，此游戏结束。用户可以输入一个新的成语，重新开始接龙\"\"\", \n",
    "limit=\"只进行成语接龙游戏，拒绝回答其他话题\")\n",
    "print(ret2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **1.6.2.使用文件来管理提示词模板**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- 便于共享\n",
    "\n",
    "\\- 便于版本管理\n",
    "\n",
    "\\- 便于存储\n",
    "\n",
    "\\- 支持常见格式(json/yaml/txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "#加载yaml格式的prompt模版\n",
    "# prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "# print(prompt.format(name=\"小黑\",what=\"恐怖的\"))\n",
    "\n",
    "# 加载json格式的prompt模版\n",
    "prompt1 = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt1.format(name=\"小红\",what=\"搞笑的\"))\n",
    "~~~\n",
    "\n",
    "~~~python\n",
    "from langchain.prompts import load_prompt\n",
    "#支持加载文件格式的模版，并且对prompt的最终解析结果进行自定义格式化\n",
    "prompt = load_prompt(\"simple_output_parser.json\")\n",
    "prompt.output_parser.parse(\n",
    "    \"chartGPT在哪一年发布的.\\n年: 2022年\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4Prompt加载支持多种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain的Prompt加载支持多种方式，以满足不同场景下的需求。以下是几种主要的加载方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 从YAML/JSON文件加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain支持从YAML文件中加载Prompt模板。YAML文件通常包含模板的类型、输入变量和模板内容等信息。加载时，可以通过LangChain提供的API读取YAML文件，并将其内容转换为Prompt对象。\n",
    "\n",
    "与YAML类似，LangChain也支持从JSON文件中加载Prompt模板。JSON文件的结构与YAML文件相似，但使用不同的语法。加载JSON文件的代码与加载YAML文件的代码类似，只是文件扩展名和可能的API调用细节可能有所不同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个名为`simple_prompt.yaml`的文件,例如其内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_type: prompt  \n",
    "input_variables: [\"adjective\", \"content\"]  \n",
    "template: Tell me a {adjective} joke about {content}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个json文件，例如其内容如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"template\": \"The {adjective} {content} make me laugh!\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载这个 YAML文件/json文件 的代码可能类似于："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "#加载yaml格式的prompt模版\n",
    "prompt = load_prompt(\"simple_prompt.yaml\")\n",
    "print(prompt.format(name=\"小黑\",what=\"恐怖的\"))\n",
    "# 加载json格式的prompt模版\n",
    "prompt1 = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt1.format(name=\"小红\",what=\"搞笑的\"))\n",
    "#支持加载文件格式的模版，并且对prompt的最终解析结果进行自定义格式化\n",
    "prompt = load_prompt(\"simple_output_parser.json\")\n",
    "prompt.output_parser.parse(\n",
    "    \"chartGPT在哪一年发布的.\\n年: 2022年\"\n",
    ")\n",
    "# 加载prompt <==> yaml\n",
    "# prompt1 = load_prompt(\"name.yaml\")\n",
    "# 加载prompt <==> json\n",
    "# prompt1 = load_prompt(\"name.json\")\n",
    "# 格式化\n",
    "print(prompt1.format(sex=\"男\",name=\"翟\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 直接使用字符串定义Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了从文件加载外，LangChain还允许直接使用字符串定义Prompt模板。这在某些情况下非常方便，尤其是当Prompt模板比较简单或者需要动态构建Prompt模板时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、Prompts高级**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 示例选择器 Example selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 介绍及应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "如果您拥有大量的示例，您可能需要选择在提示中包含哪些示例。ExampleSelector 是负责执行此操作的类。 基本接口定义如下所示：\n",
    "\n",
    "```text\n",
    "class BaseExampleSelector(ABC):\n",
    "    \"\"\"Interface for selecting examples to include in prompts.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "        \"\"\"Select which examples to use based on the inputs.\"\"\"\n",
    "```\n",
    "\n",
    "它只需要暴露一个 `select_examples` 方法。该方法接收输入变量并返回一个示例列表。具体如何选择这些示例取决于每个具体实现。\n",
    "\n",
    "LangChain的示例选择器是一个用于从一组示例中动态选择部分示例以构建提示（prompt）的重要组件。在LangChain中，有多种不同类型的示例选择器，包括自定义样例选择器、长度样例选择器、MMR样例选择器、n-gram重叠度样例选择器和相似度样例选择器。以下是这些示例选择器的简要介绍和格式清晰的回答：\n",
    "\n",
    "1. 自定义样例选择器\n",
    "   - 允许用户根据自己的业务逻辑和需求来定义样例的选择方式。\n",
    "   - 至少需要实现两个方法：`add_example`（用于添加新示例）和`select_examples`（基于输入变量返回一个样例列表）。\n",
    "   - 用户可以通过继承`BaseExampleSelector`类并实现所需的方法来创建自定义样例选择器。\n",
    "2. 长度样例选择器\n",
    "   - 根据输入的长度来选择要使用的示例。对于较长的输入，它会选择较少的示例，而对于较短的输入，则会选择更多示例。\n",
    "   - 这在构建提示时需要控制上下文窗口长度时特别有用。\n",
    "   - 使用时，可以导入`LengthBasedExampleSelector`类，并传入示例列表来创建长度样例选择器。\n",
    "3. MMR样例选择器\n",
    "   - MMR（Maximum Marginal Relevance）是一种常用于信息检索和推荐系统的排序算法，也适用于样例选择。\n",
    "   - MMR样例选择器通过计算示例与输入之间的相关性以及示例之间的不相似性来选择样例。\n",
    "   - 具体的实现细节可能因库或框架而异，但通常用户需要指定计算相关性和不相似性的方法。\n",
    "4. n-gram重叠度样例选择器\n",
    "   - 基于输入和示例之间的n-gram重叠度来选择样例。\n",
    "   - n-gram是一种用于表示文本中连续n个词或字符的序列的模型。\n",
    "   - 通过计算输入和示例之间的n-gram重叠度，可以选择与输入内容最相关的样例。\n",
    "5. 相似度样例选择器\n",
    "   - 基于输入和示例之间的相似度来选择样例。\n",
    "   - 相似度可以使用各种方法来计算，如余弦相似度、Jaccard相似度等。\n",
    "   - 选择与输入内容最相似的样例可以帮助语言模型更好地理解prompt，并给出更准确的回答。\n",
    "\n",
    "**归纳**：\n",
    "\n",
    "LangChain的示例选择器为构建有效的提示提供了灵活的工具。用户可以根据具体的应用场景和需求选择合适的示例选择器。无论是根据长度、MMR算法、n-gram重叠度还是相似度来选择样例，都可以帮助提高语言模型的理解能力和回答准确性。同时，用户还可以根据自己的业务逻辑自定义样例选择器，以实现更精细的样例选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2自定义示例选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们将创建一个自定义示例选择器，该选择器从给定的示例列表中选择每个交替示例。\n",
    "\n",
    "`ExampleSelector` 必须实现两个方法：\n",
    "\n",
    "1. `add_example` 方法，接受一个示例并将其添加到 ExampleSelector 中\n",
    "2. `select_examples` 方法，接受输入变量（用于用户输入）并返回要在 few shot prompt 中使用的示例列表。\n",
    "\n",
    "代码实现\n",
    "\n",
    "1.新建一张表\n",
    "\n",
    "标题    简单描述\n",
    "\n",
    "流感       发烧、咳嗽、喉咙痛、身体酸痛  \n",
    "\n",
    "2.写一个vue页面，搜索框，输入标题\n",
    "\n",
    "3.写一个接口，在接口中获取到输入信息，用自定义选择器实现，查询数据库中的数据构造examples,\n",
    "\n",
    "4.生成prompt,调用大模型，返回结果\n",
    "\n",
    "5.vue展示，sse流式显示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "class MedicalExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        初始化医疗示例选择器\n",
    "        :param examples: 医疗相关的示例列表，每个示例是一个字典，包含输入和输出\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example: Dict[str, str]) -> None:\n",
    "        \"\"\"将新示例添加到存储中的键。\"\"\"\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[Dict[str, str]]:  \n",
    "        \"\"\"  \n",
    "        根据输入变量选择医疗示例  \n",
    "        :param input_variables: 包含选择条件（如疾病名称、症状等）的字典\n",
    "        :return: 符合条件的示例列表\n",
    "        \"\"\"\n",
    "        # 假设我们根据疾病名称来选择示例\n",
    "        disease_name = input_variables.get(\"disease_name\", None)\n",
    "        if disease_name is None:\n",
    "            return []  # 如果没有提供疾病名称，则返回空列表\n",
    "        selected_examples = [\n",
    "            example for example in self.examples\n",
    "            if disease_name.lower() in example[\"input\"].lower()\n",
    "        ]\n",
    "        return selected_examples\n",
    "# 示例数据\n",
    "examples = [\n",
    "{\"input\":\"流感症状\",\"output\":\"发烧、咳嗽、喉咙痛、身体酸痛。\"},\n",
    "{\"input\":\"糖尿病治疗\",\"output\":\"饮食控制、运动、药物\"},\n",
    "{\"input\":\"新冠肺炎症状\",\"output\":\"发烧、咳嗽、疲劳、味觉或嗅觉丧失。\"},\n",
    "{\"input\":\"糖尿病症状\",\"output\":\"口渴、尿频、体重减轻。\"},\n",
    "]\n",
    "# 创建示例选择器实例\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "# 创建医疗示例选择器实例\n",
    "medical_example_selector = MedicalExampleSelector(examples)\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # 我们提供一个ExampleSelector而不是示例。\n",
    "    example_selector=medical_example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"根据描述确定病情\",\n",
    "    suffix=\"输入: {disease_name}\\n输出:\",\n",
    "    input_variables=[\"disease_name\"],\n",
    ")\n",
    "# 一个输入较小的示例，因此它选择所有示例。\n",
    "dynamic_prompt.format(disease_name=\"糖尿病\")\n",
    "print(prompt)\n",
    "tongyi = Tongyi()\n",
    "ret = tongyi.invoke(prompt)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3根据长度选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个示例选择器根据长度选择要使用的示例。当您担心构建的提示会超过上下文窗口的长度时，这是非常有用的。对于较长的输入，它会选择较少的示例进行包含，而对于较短的输入，它会选择更多的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate  \n",
    "from langchain.prompts import FewShotPromptTemplate  \n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector \n",
    "# 导入通义大模型\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "\n",
    "examples = [  \n",
    "    {\"input\": \"患者如何在家测量血压？\", \"output\": \"患者可以在家使用电子血压计测量血压，遵循说明书上的步骤，通常在早上和晚上各测量一次。\"},  \n",
    "    {\"input\": \"糖尿病患者的饮食应该注意什么？\", \"output\": \"糖尿病患者应该注意饮食中的糖分和碳水化合物摄入，多食用蔬菜、全谷物和瘦肉，避免高糖和高脂肪食物。\"},  \n",
    "    {\"input\": \"儿童发烧时应该如何处理？\", \"output\": \"儿童发烧时，应首先测量体温，如果超过38.5°C，可以使用退烧药，并给孩子多喝水，保持通风，适当减少衣物。\"}  \n",
    "]  \n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "# 定义计算长度的函数  \n",
    "def calculate_text_length(text):  \n",
    "    return len(re.split(\"\\n| \", text))  \n",
    "    \n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # 这些是可供选择的示例。\n",
    "    examples=examples,\n",
    "    # 这是用于格式化示例的PromptTemplate。\n",
    "    example_prompt=example_prompt,\n",
    "    # 这是格式化示例的最大长度。\n",
    "    # 长度由下面的get_text_length函数测量。\n",
    "    max_length=10,\n",
    "    \n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # 我们提供一个ExampleSelector而不是示例。\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"根据描述确定病情\",\n",
    "    suffix=\"输入: {adjective}\\n输出:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# 一个输入较小的示例，因此它选择所有示例。\n",
    "prompt = dynamic_prompt.format(adjective=\"糖尿病患者的饮食\")\n",
    "\n",
    "# 一个输入较长的示例，因此它只选择一个示例。\n",
    "# long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "# print(dynamic_prompt.format(adjective=long_string))\n",
    "\n",
    "# 您还可以将示例添加到示例选择器中。\n",
    "# new_example = {\"input\": \"big\", \"output\": \"small\"}\n",
    "# dynamic_prompt.example_selector.add_example(new_example)\n",
    "# print(dynamic_prompt.format(adjective=\"enthusiastic\"))\n",
    "\n",
    "tongyi = Tongyi()\n",
    "print(prompt)\n",
    "ret = tongyi.invoke(prompt)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4最大边际相关性（MMR）选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "``根据示例与输入之间的相似性以及多样性进行选择。它通过找到与输入具有最大余弦相似度的嵌入示例，并在迭代中添加它们，同时对已选择示例的接近程度进行惩罚来实现这一目标。\n",
    "\n",
    "\\- MMR是一种在信息检索中常用的方法，它的目标是在相关性和多样性之间找到一个平衡\n",
    "\n",
    "\\- MMR会首先找出与输入最相似（即余弦相似度最大）的样本\n",
    "\n",
    "\\- 然后在迭代添加样本的过程中，对于与已选择样本过于接近（即相似度过高）的样本进行惩罚\n",
    "\n",
    "\\- MMR既能确保选出的样本与输入高度相关，又能保证选出的样本之间有足够的多样性\n",
    "\n",
    "\\- 关注如何在相关性和多样性之间找到一个平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "#假设已经有这么多的提示词示例组：\n",
    "examples =  [  \n",
    "    {\"id\": \"1\", \"features\": ', '.join([\"时尚\", \"运动鞋\", \"跑步\"])},  \n",
    "    {\"id\": \"2\", \"features\": ', '.join([\"休闲\", \"运动鞋\", \"篮球\"])}, \n",
    "    {\"id\": \"3\", \"features\": ', '.join([\"商务\", \"皮鞋\", \"正装\"])}, \n",
    "    {\"id\": \"4\", \"features\": ', '.join([\"户外\", \"徒步鞋\", \"探险\"])}, \n",
    "    {\"id\": \"5\", \"features\": ', '.join([\"时尚\", \"板鞋\", \"滑板\"])}, \n",
    "]  \n",
    "\n",
    "#构造提示词模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"id\",\"features\"],\n",
    "    template=\"id：{id}\\n描述：{features}\"\n",
    ")\n",
    "\n",
    "#调用MMR\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    #传入示例组\n",
    "    examples,\n",
    "    #使用阿里云的dashscope的嵌入来做相似性搜索\n",
    "    DashScopeEmbeddings(),\n",
    "    #设置使用的向量数据库是什么\n",
    "    FAISS,\n",
    "    #结果条数\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "#使用小样本提示词模版来实现动态示例的调用\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"查询和features相似度最高的id\",\n",
    "    suffix=\"关键词为：{word}\",\n",
    "    input_variables=[\"word\"]\n",
    ")\n",
    "\n",
    "print(dynamic_prompt.format(word=\"时尚\"))\n",
    "\n",
    "# qq = dynamic_prompt.format(word=\"时尚\")\n",
    "# llm = Tongyi()\n",
    "# llm.invoke(qq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5**最大余弦相似度**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度（Cosine Similarity）是一种常用的度量两个非零向量之间相似度的方法，广泛应用于文本挖掘、推荐系统、信息检索等领域。其原理及公式如下：\n",
    "\n",
    "余弦相似度通过计算两个向量之间的夹角的余弦值来评估它们的相似度。在向量空间中，两个向量的夹角越小，说明它们的方向越接近，从而相似度越高。余弦相似度与向量的长度无关，仅与向量的方向有关。\n",
    "\n",
    "- 一种常见的相似度计算方法\n",
    "\n",
    "- 它通过计算两个向量（在这里，向量可以代表文本、句子或词语）之间的余弦值来衡量它们的相似度\n",
    "\n",
    "- 余弦值越接近1，表示两个向量越相似\n",
    "\n",
    "- 主要关注的是如何准确衡量两个向量的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "#假设已经有这么多的提示词示例组：\n",
    "examples =  [  \n",
    "    {\"id\": \"1\", \"features\": ', '.join([\"时尚\", \"运动鞋\", \"跑步\"])},  \n",
    "    {\"id\": \"2\", \"features\": ', '.join([\"休闲\", \"运动鞋\", \"篮球\"])}, \n",
    "    {\"id\": \"3\", \"features\": ', '.join([\"商务\", \"皮鞋\", \"正装\"])}, \n",
    "    {\"id\": \"4\", \"features\": ', '.join([\"户外\", \"徒步鞋\", \"探险\"])}, \n",
    "    {\"id\": \"5\", \"features\": ', '.join([\"时尚\", \"板鞋\", \"滑板\"])}, \n",
    "]  \n",
    "\n",
    "#构造提示词模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"id\",\"features\"],\n",
    "    template=\"id：{id}\\n描述：{features}\"\n",
    ")\n",
    "\n",
    "#调用MMR\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    #传入示例组\n",
    "    examples,\n",
    "    #使用阿里云的dashscope的嵌入来做相似性搜索\n",
    "    DashScopeEmbeddings(),\n",
    "    #设置使用的向量数据库是什么\n",
    "    FAISS,\n",
    "    #结果条数\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "#使用小样本提示词模版来实现动态示例的调用\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"查询和features相似度最高的id\",\n",
    "    suffix=\"关键词为：{word}\",\n",
    "    input_variables=[\"word\"]\n",
    ")\n",
    "\n",
    "print(dynamic_prompt.format(word=\"时尚\"))\n",
    "\n",
    "# qq = dynamic_prompt.format(word=\"时尚\")\n",
    "# llm = Tongyi()\n",
    "# llm.invoke(qq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3输出解析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain输出解析器是LangChain框架中的一个重要组成部分，它负责将自然语言模型（如ChatGPT、GPT系列等）的输出从文本形式转换为结构化的数据格式，以便进行进一步的分析或操作。以下是对LangChain输出解析器的详细解析：\n",
    "\n",
    "一、定义与功能\n",
    "\n",
    "LangChain输出解析器是一种特殊的自然语言处理（NLP）技术，其核心功能是将非结构化的自然语言文本转换为结构化的数据，如JSON对象、列表等。这种转换使得输出更加易于程序理解和处理，提高了数据处理的效率和准确性。\n",
    "\n",
    "二、工作原理\n",
    "\n",
    "LangChain输出解析器的工作原理通常包括以下几个步骤：\n",
    "\n",
    "1. **文本预处理**：对输入的自然语言文本进行预处理，包括去除停用词、标点符号等无意义字符，以及对文本进行分词、词性标注等操作。\n",
    "2. **特征提取**：将预处理后的文本转换为向量表示，以便进行后续处理。这可以通过词袋模型、TF-IDF、Word2Vec等方法实现。\n",
    "3. **模型解析**：将处理后的文本输入到训练好的输出解析器模型中，模型根据预设的规则或算法将文本解析为结构化的数据格式。\n",
    "4. **输出结果**：输出解析器将解析后的结构化数据以特定的格式（如JSON、列表等）返回给调用者。\n",
    "\n",
    "三、类型与实现\n",
    "\n",
    "LangChain提供了多种类型的输出解析器，以满足不同的需求。这些解析器包括但不限于：\n",
    "\n",
    "1. **结构化输出解析器**：将文本解析为JSON对象等结构化数据格式。\n",
    "2. **CSV解析器**：将文本解析为CSV格式的数据，便于进行表格化处理。\n",
    "3. **日期时间解析器**：专门用于处理日期和时间相关的输出，确保输出的日期和时间格式正确。\n",
    "4. **枚举解析器**：将文本中的特定词汇映射为枚举类型的值，适用于处理有限集合的数据。\n",
    "5. **Pydantic解析器**：利用Pydantic库的功能，将文本解析为符合特定数据模式的Python对象。\n",
    "\n",
    "四、应用场景\n",
    "\n",
    "LangChain输出解析器在多个领域都有广泛的应用，包括但不限于：\n",
    "\n",
    "1. **问答系统**：从用户的问题中抽取关键信息，并将其作为查询条件在知识库中进行搜索。\n",
    "2. **机器翻译**：将源语言文本解析为目标语言文本中的实体、关系等信息，以提高翻译的准确性和流畅性。\n",
    "3. **摘要生成**：从原始文本中抽取关键信息，并将其组合成一个简洁的摘要。\n",
    "4. **数据分析**：将自然语言形式的数据转换为结构化数据，以便进行进一步的数据分析和挖掘。\n",
    "\n",
    "五、总结\n",
    "\n",
    "LangChain输出解析器是LangChain框架中一项强大的功能，它能够将自然语言模型的输出转换为结构化的数据格式，为后续的处理和分析提供了极大的便利。通过不同的解析器类型，LangChain能够满足不同领域和场景下的需求，为构建高效的自然语言处理应用程序提供了有力的支持。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2列表解析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当您想要返回逗号分隔的项目列表时，可以使用此输出解析器。也可以作为CVS解析器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser,NumberedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 定义列表解析器\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "# 获取格式说明\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "# 创建一个提示模板\n",
    "promptTemplate = PromptTemplate(\n",
    "    template=\"列出五种{subject}。\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "# 测试\n",
    "prompt=promptTemplate.format(subject=\"水果\")\n",
    "ret = llm.invoke(prompt)\n",
    "output_parser.parse(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3日期时间解析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个OutputParser展示了如何将LLM的输出解析为日期时间格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.datetime import DatetimeOutputParser\n",
    "\n",
    "# 1\\实例化\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"请按下面要求回答问题:\n",
    "{question}\n",
    "{format_instructions}\"\"\"\n",
    "# 2、定义模板\n",
    "promptTemplate = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "# 3、生成prompt\n",
    "prompt = promptTemplate.format(question=\"新中国成立的日期是什么？\")\n",
    "# 4、提交大模型\n",
    "output = llm.invoke(prompt)\n",
    "# 5、解析\n",
    "res = output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 枚举解析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**枚举解析器在 LangChain 中的可能实现和功能**：\n",
    "\n",
    "1. 定义\n",
    "   - 枚举解析器可能是 LangChain 输出解析器的一个特定实现，专门用于处理枚举类型的数据或输出。\n",
    "2. 功能\n",
    "   - **识别枚举值**：从模型输出中识别并提取枚举值。\n",
    "   - **格式化输出**：将提取的枚举值转换为指定的数据结构或格式。\n",
    "   - **验证**：可能还包括对提取的枚举值进行验证，以确保其符合预期的枚举类型。\n",
    "3. 核心方法\n",
    "   - `get_format_instructions`：返回指导如何格式化枚举类型输出的字符串。\n",
    "   - `parse_enum`（假设的方法名）：接受一个字符串（模型输出），并尝试从中解析出枚举值。\n",
    "   - `validate_enum`（假设的方法名）：验证解析出的枚举值是否有效。\n",
    "4. 实现\n",
    "   - 枚举解析器可能需要预定义或动态加载一个枚举类型的定义（如 Python 中的 `Enum` 类），以便能够正确解析和验证枚举值。\n",
    "   - 它可能会使用正则表达式、字符串匹配或其他 NLP 技术来从模型输出中提取枚举值。\n",
    "5. 使用场景\n",
    "   - 当模型输出中包含枚举类型的数据时，枚举解析器可以确保这些数据被正确地提取、格式化和验证。\n",
    "   - 这在处理具有固定选项集（如颜色、状态、类别等）的数据时特别有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "class Colors(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "    BLUE = \"blue\"\n",
    "parser = EnumOutputParser(enum=Colors)\n",
    "print(parser.parse(\"red\"))\n",
    "# -> <Colors.RED: 'red'>\n",
    "# 包括空格也可以\n",
    "print(parser.parse(\" green\"))\n",
    "# -> <Colors.GREEN: 'green'>\n",
    "# 包括\\n\\t也可以\n",
    "print(parser.parse(\"blue\\n\\t\"))\n",
    "# -> <Colors.BLUE: 'blue'>\n",
    "# 其他字符不可以\n",
    "print(parser.parse(\"yellow\"))\n",
    "# -> <Colors.BLUE: 'blue'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5 Pydantic 解析器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用 Pydantic 解析器，必须保证大模型的输出是可靠的Json格式。\n",
    "\n",
    "这个解析器将大模型输出的Json字符串解析为Pydantic模型。在信息提取的场景中很有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "\n",
    "# 定义您期望的数据结构。\n",
    "class Joke(BaseModel):\n",
    "    joke: str = Field(description=\"设置笑话的问题部分\",title=\"笑话\")\n",
    "    answer: str = Field(description=\"解答笑话的答案部分\")\n",
    "# 设置一个解析器 + 将指令注入到提示模板中。\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "promptTemplate = PromptTemplate(\n",
    "    template=\"回答用户的问题：\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "prompt = promptTemplate.format(query=\"讲一个中文的笑话。\")\n",
    "ret = llm.invoke(prompt)\n",
    "parser.parse(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.6 JSON 解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# 定义您期望的数据结构。\n",
    "class Joke(BaseModel):\n",
    "    joke: str = Field(description=\"设置笑话的问题部分\")\n",
    "    answer: str = Field(description=\"解答笑话的答案部分\")\n",
    "# 还有一个用于提示语言模型填充数据结构的查询意图。\n",
    "joke_query=\"告诉我一个笑话\"\n",
    "# 设置一个解析器 + 将指令注入到提示模板中。\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "promptTemplate = PromptTemplate(\n",
    "    template=\"回答用户的问题：\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "prompt = promptTemplate.format(query=joke_query)\n",
    "ret = llm.invoke(prompt)\n",
    "parser.parse(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "class MedicalExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples: List[Dict[str, str]]):\n",
    "        \"\"\" 初始化菜品示例选择器 :param examples: 菜品相关的示例列表，每个示例是一个字典，包含输入和输出 \"\"\"\n",
    "        self.examples = examples\n",
    "    def add_example(self, example: Dict[str, str]) -> None:\n",
    "        \"\"\" 将新示例添加到存储中的键。\"\"\"\n",
    "        self.examples.append(example)\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        \"\"\" 根据输入变量选择菜品示例 :param input_variables: 包含选择条件（如菜品名称、菜品种类等）的字典 :return: 符合条件的示例列表 \"\"\"\n",
    "        # 假设我们根据疾病名称来选择示例\n",
    "        disease_name = input_variables.get(\"disease_name\", None)\n",
    "        if disease_name is None:\n",
    "            return []  # 如果没有提供疾病名称，则返回空列表\n",
    "        selected_examples = [\n",
    "            example for example in self.examples\n",
    "            if disease_name.lower() in example[\"input\"].lower()\n",
    "        ]\n",
    "        return selected_examples\n",
    "# 示例数据\n",
    "examples = [\n",
    "{\"input\":\"菜品名称\",\"output\":\" 宫保鸡丁，糖醋排骨，红烧肉，酸辣粉，酸辣土豆丝，酸辣鱼片，酸辣虾，酸辣蟹\"},\n",
    "{\"input\":\"菜品种类\",\"output\":\"热菜，凉菜，主食 ，饮品 ，小吃\"},\n",
    "{\"input\":\"菜品菜系\",\"output\":\" 川菜，粤菜，鲁菜，浙菜，湘菜，苏菜，徽菜，闽菜，鲁菜，湘菜，苏菜，徽菜，闽菜，鲁菜，湘菜，苏菜，徽菜，闽菜，鲁菜，湘菜，苏菜，徽菜，闽菜，鲁菜，湘菜，苏菜，徽菜，闽菜，鲁菜，湘菜，苏菜\"},\n",
    "]\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "# 创建医疗示例选择器实例\n",
    "medical_example_selector = MedicalExampleSelector(examples)\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # 我们提供一个ExampleSelector而不是示例\n",
    "    example_selector=medical_example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"根据描述说出菜品\",\n",
    "    suffix=\"输入: {disease_name}\\n输出:\",\n",
    "    input_variables=[\"disease_name\"],\n",
    ")\n",
    "# 一个输入较小的示例，因此它选择所有示例\n",
    "prompt=dynamic_prompt.format(disease_name=\"菜品\")\n",
    "ret = llm.invoke(prompt)\n",
    "output_parser.parse(ret)\n",
    "parser.parse(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、RAG增强**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 RAG增强检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 RAG增强检索概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务重新训练模型。\n",
    "- **优化模型输出效果**：用户可以额外附加外部知识库，丰富输入，从而优化模型的输出效果。\n",
    "- **减少模型幻觉**：通过结合外部知识库，RAG能够生成更准确、更符合上下文的答案，减少模型生成的错误或不正确内容。\n",
    "\n",
    "5. 应用场景\n",
    "\n",
    "RAG技术在知识密集型NLP任务中表现出色，如问答系统、文本摘要、对话系统等。在这些任务中，RAG能够提供更准确、更全面的文本处理能力，满足用户的不同需求。\n",
    "\n",
    "RAG的流程如下：\n",
    "\n",
    "\\- 1文档加载\n",
    "\n",
    "\\- 2文档分割\n",
    "\n",
    "\\- 3文档向量化\n",
    "\n",
    "-4构建知识库\n",
    "\n",
    "\\- 5基于知识库的问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 文档加载的几种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.1 加载markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./NBA/demo.md\", encoding='utf-8')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2 加载csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"./NBA/demo.csv\", encoding=\"utf-8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.3 加载html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用loader来加载cvs文件\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "loader = BSHTMLLoader(\"./NBA/demo.html\", open_encoding=\"utf-8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.4 加载JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用loader来加载json文件\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path = \"simple_prompt.json\",\n",
    "    jq_schema=\".\",\n",
    "    text_content=True\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.5 加载PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用loader来加载pdf文件\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"loader.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.6 加载doc/docx/wps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"doc/demo.docx\")\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.6 加载xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "loader = UnstructuredExcelLoader(\"doc/demo.xlsx\",mode=\"elements\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputmes = \"请写一首关于春天的诗\"  \n",
    "  \n",
    "stream = llm.stream(inputmes)\n",
    "# 注意，我假设 prompt 是正确的参数名，而不是 promt  \n",
    "# for chunk in stream:  \n",
    "#     print(chunk)\n",
    "# 假设 chunk 对象有一个 text 属性，这取决于 Tongyi 的返回类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2流式调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1流式调用概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "大模型的流式输出方法主要涉及到在模型生成答案的过程中，以流的形式将结果逐步输出给用户，而不是等待整个答案生成完成后再一次性输出。这种流式输出的方法有助于提高用户体验，尤其是在处理大规模数据和生成长文本时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2案例实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 初始化文本分割器\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    chunk_size=200,# 切分的文本块大小，一般通过长度函数来计算\n",
    "    chunk_overlap=0# 切分的文本块重叠大小，一般通过长度函数来计算\n",
    "    )\n",
    "loader = TextLoader('../data/example.txt', encoding='utf-8')\n",
    "text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3token跟踪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3token跟踪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在LLM（大型语言模型）中，Token追踪的概念并不直接等同于传统的资产追踪或位置追踪。但在LLM的语境下，Token是模型进行语言处理的基本信息单元，它代表了模型可以理解和生成的最小意义单位。关于LLM的Token追踪，我们可以从以下几个方面进行理解：\n",
    "\n",
    "1. Token的定义\n",
    "   - Token是大语言模型（LLM）进行语言处理的最小单元，它可以是一个字、一个词，甚至是一个短语或句子的一部分。\n",
    "   - Token在不同的上下文中可能会有不同的划分粒度，这取决于所使用的特定标记化（Tokenization）方案。\n",
    "2. Token的追踪意义\n",
    "   - 在LLM中，Token的追踪并不是指追踪某个具体的Token的物理位置或来源，而是指模型如何处理、理解和生成这些Token。\n",
    "   - 当用户与LLM进行交互时，输入的文本会被转换为一系列的Token，这些Token随后被模型用于生成预测或回答。\n",
    "3. Token的数量与关系\n",
    "   - 一个Token的大致长度可以用字符或单词来估算。例如，在英语中，一个Token大约等于4个字符，而在中文中，一个Token可能对应于¾个单词（这是一个粗略的估算）。\n",
    "   - 模型的性能通常与其能够处理的Token数量有关。一些模型具有MAX TOKENS的参数，这限制了模型在一次会话中能够基于整个上下文记忆的Token数量。\n",
    "4. Token与向量\n",
    "   - Token在LLM中通常与向量相关联，这些向量代表了Token的特征或语义信息。\n",
    "   - 相同的Token在不同的上下文中可能会有不同的特征向量，这取决于其在整个文本中的角色和含义。\n",
    "5. Token在LLM中的作用\n",
    "   - Token作为原始文本数据和LLM可以使用的数字表示之间的桥梁，确保了文本的连贯性和一致性。\n",
    "   - LLM使用Token来理解和生成文本，有效地处理各种任务，如写作、翻译和回答查询。\n",
    "\n",
    "综上所述，LLM的Token追踪更多地是指模型如何处理、理解和生成这些基本的语言单元，而不是物理上追踪某个Token的位置或来源。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2案例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.4文档分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 原理\n",
    "\n",
    "\\1. 将文档分成小的、有意义的块(句子).\n",
    "\n",
    "\\2. 将小的块组合成为一个更大的块，直到达到一定的大小.\n",
    "\n",
    "\\3. 一旦达到一定的大小，接着开始创建与下一个块重叠的部分.\n",
    "\n",
    "分类\n",
    "\n",
    "\\- 按字符切割\n",
    "\n",
    "\\- 代码文档切割\n",
    "\n",
    "\\- 按token来切割\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1字符串分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用langchain的text_splitter模块，导入CharacterTextSplitter类\n",
    "\n",
    "\\- separator 分隔符\n",
    "\n",
    "\\- chunk_size 每块的最大长度\n",
    "\n",
    "\\- chunk_overlap 重复字符的长度\n",
    "\n",
    "关于 `chunk_overlap` 参数，它指的是在分割文本时，相邻的两个文本块之间将有多少字符是重叠的。这个参数在某些情况下非常有用，比如当你想要确保某些关键信息（如句子的一部分）不会恰好被分割在两个不同的块中时。通过设置重叠，你可以增加相邻块之间的上下文联系，这有助于后续处理（如文本摘要、情感分析等）时保持更好的连贯性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 实例化TextLoader对象\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding=\"utf-8\")\n",
    "# 加载文档\n",
    "docs = loader.load()\n",
    "print(docs)\n",
    "\n",
    "# 导入分割类\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 实例化\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=150, chunk_overlap=0)\n",
    "\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 按token来切割文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiktoken怎么计算分块的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#初始化切分器\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, #切分的文本块大小，一般通过长度函数计算\n",
    "    chunk_overlap=0, #切分的文本块重叠大小，一般通过长度函数计算\n",
    ")\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding=\"utf-8\")\n",
    "# 加载文档\n",
    "loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator='\\n',chunk_size=200,chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 代码文档切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "\n",
    "#支持解析的编程语言\n",
    "#[e.value for e in Language]\n",
    "#要切割的代码文档\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "#调用函数\n",
    "hello_world()\n",
    "\"\"\"\n",
    "py_spliter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "py_spliter.create_documents([PYTHON_CODE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5文档向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1向量模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DashScopeEmbeddings\n",
    "\n",
    "DashScopeEmbeddings 是一种服务，主要用于生成和处理文本嵌入向量。以下是关于 DashScopeEmbeddings 的详细介绍：\n",
    "\n",
    "1. 服务概述\n",
    "   - DashScopeEmbeddings 是一种基于阿里云百炼大模型服务平台的文本嵌入服务。\n",
    "   - 它通过标准的API提供多种模型服务，支持文本Embedding的模型，其中文名为通用文本向量，英文名为text-embedding-v1。\n",
    "   - 该服务允许用户方便地通过DashScope API调用来获得输入文本的embedding向量。\n",
    "2. 前提条件\n",
    "   - 需要开通阿里云百炼大模型服务产品。\n",
    "   - 需要创建API_KEY以获取API-KEY。\n",
    "3. 安装环境\n",
    "   - 需要安装Python，版本要求在3.8到3.12之间。\n",
    "   - 需要通过pip安装相关的库，如`pip install llama-index-core` 和 `pip install llama-index-embeddings-dashscope`。\n",
    "4. 使用示例\n",
    "   - 示例代码展示了如何在Llama-Index中调用DashScopeEmbedding服务。\n",
    "   - 需要使用API-KEY替换示例中的占位符`YOUR_DASHSCOPE_API_KEY`，以便代码能够正常运行。\n",
    "   - 代码会创建一个`DashScopeEmbedding`对象，并调用其`get_text_embedding_batch`方法来获取一批文本的embedding向量。\n",
    "5. 应用场景\n",
    "   - DashScopeEmbeddings 可用于各种需要文本向量表示的场景，如语义搜索、文本分类、信息检索等。\n",
    "   - 通过在向量空间中表示文本，可以进行高效的语义相似度计算，从而找到在语义上最相似的文本片段。\n",
    "6. 注意事项\n",
    "   - 在使用DashScopeEmbeddings服务时，需要确保遵循相关的服务条款和使用限制。\n",
    "   - 对于大规模的文本处理任务，可能需要考虑性能优化和资源管理。\n",
    "\n",
    "总结来说，DashScopeEmbeddings 是一种功能强大的文本嵌入服务，通过它可以轻松地生成和处理文本的embedding向量，为各种自然语言处理任务提供有力的支持。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个方法：(通过父类查看)\n",
    "\n",
    "\\- `embed_documents`: 输入一个文档列表，返回一个二维数组，每个元素是一个文档的向量。\n",
    "\n",
    "\\- `embed_query`: 输入一个查询，返回一个向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "\n",
    "embeddings = DashScopeEmbeddings()\n",
    "embeddings.embed_documents([\"你好\"])\n",
    "ret = embeddings.embed_query(\"你是谁？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2 BGE\\BCE\\M3E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.1 Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Chroma向量数据库（也称为ChromaDB）是一个开源的向量数据库，主要用于AI和机器学习场景。以下是关于Chroma的详细介绍：\n",
    "\n",
    "1. 主要功能\n",
    "   - ChromaDB主要用于存储和查询向量数据，这些数据通常是通过嵌入（embedding）算法从文本、图像等数据转换而来的。\n",
    "   - 它的设计目标是简化大模型应用的构建过程，允许开发者轻松地将知识、事实和技能等文档整合进大型语言模型（LLM）中。\n",
    "2. 特点\n",
    "   - **轻量级**：ChromaDB是一个基于向量检索库实现的轻量级向量数据库。\n",
    "   - **易用性**：提供简单的API，易于集成和使用。\n",
    "   - **功能丰富**：支持存储嵌入及其元数据、嵌入文档和查询、搜索嵌入等功能。\n",
    "   - **集成**：可以直接插入LangChain、LlamaIndex、OpenAI等。\n",
    "   - **多语言支持**：包括Python和JavaScript客户端SDK。\n",
    "   - **开源**：采用Apache 2.0开源许可。\n",
    "3. 性能\n",
    "   - Chroma使用高效的索引结构，如倒排索引、KD-树或基于图的索引，以加快向量搜索速度。\n",
    "   - 它支持多种向量相似度度量标准，包括欧氏距离、余弦相似度等，使其可以广泛应用于各种场景。\n",
    "4. 使用方式\n",
    "   - Chroma提供了多种使用方式，包括内存模式、client模式和Server模式，以及支持数据持久化的功能。\n",
    "   - 开发者可以通过简单的API调用，创建数据集、写入数据、查询数据等。\n",
    "5. 限制\n",
    "   - 目前只支持CPU计算，不支持GPU加速。\n",
    "   - 功能相对简单，但计划未来推出托管产品，提供无服务器存储和检索功能，支持向上和向下扩展。\n",
    "\n",
    "总的来说，Chroma向量数据库为研究人员和开发者提供了一个有用的工具，使他们能够利用词向量来处理自然语言数据并改善各种NLP任务的性能。同时，其轻量级、易用性和功能丰富的特点也使得它成为许多项目的首选。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入向量化的类\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc = TextLoader(\"./doc/NBA新闻.txt\",encoding='utf-8').load()\n",
    "spliter = CharacterTextSplitter(\"\\n\",chunk_size=1000, chunk_overlap=0)\n",
    "chunks = spliter.split_documents(doc)\n",
    "# 实例化\n",
    "embeddings = DashScopeEmbeddings()\n",
    "# 创建向量数据库,向量化文档\n",
    "db = Chroma.from_documents(chunks,embeddings, persist_directory=\"./chroma\")\n",
    "db.persist()\n",
    "# 添加文档\n",
    "doc = TextLoader(\"./doc/NBA新闻.txt\",encoding='utf-8').load()\n",
    "spliter = CharacterTextSplitter(\"\\n\",chunk_size=200, chunk_overlap=0)\n",
    "chunks = spliter.split_documents(doc)\n",
    "# 添加文档的方法\n",
    "db.add_documents(chunks)\n",
    "db.__len__()\n",
    "# 对数据进行加载\n",
    "db1 = Chroma(persist_directory=\"./chroma/zhisk1\", embedding_function=embeddings)\n",
    "db1.__len__()\n",
    "# 召回相似的数据块\n",
    "rets = db.similarity_search(\"2024冠军球队是谁\",k=2)\n",
    "# 直接拼接prompt\n",
    "prompt = \"\"\n",
    "for ret in rets:\n",
    "    prompt+=ret.page_content +  \"\\n\"\n",
    "prompt += \"请根据上面内容回答：\"+\"2024冠军球队是谁\"\n",
    "print(prompt)\n",
    "\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "llm = Tongyi()\n",
    "llm.invoke(prompt)\n",
    "\n",
    "from langchain_community.llms import Tongyi\n",
    "# 检索问答\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = Tongyi()\n",
    "# 实例化\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "print(qa.invoke(\"2024年NBA冠军是谁\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.2 Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss向量数据库，实际上是Facebook AI Research团队开源的一个高性能的向量相似性搜索库，主要用于处理大规模向量数据集并快速搜索最相似的向量。以下是关于Faiss的详细概述：\n",
    "\n",
    "1. 基本信息\n",
    "\n",
    "- **开发单位**：Facebook AI Research\n",
    "- **主要功能**：在大规模向量数据集中进行快速相似性搜索\n",
    "- **应用领域**：广泛应用于图像搜索、文本搜索、推荐系统等\n",
    "\n",
    "2. 主要特性\n",
    "\n",
    "- **高性能**：Faiss通过优化索引结构和并行计算能力，在大规模数据上实现快速搜索。\n",
    "- **多平台支持**：支持CPU和GPU两种模式，GPU模式可利用CUDA进行加速。\n",
    "- **多种索引类型**：支持Flat、IVF、PQ等多种索引类型，以适应不同场景下的需求。\n",
    "- **灵活性**：可以根据数据集大小和维度选择合适的算法，以及实现代价最高的计算步骤在GPU上解决线性计算问题。\n",
    "\n",
    "3. 工作原理\n",
    "\n",
    "- **核心思想**：将向量空间嵌入到更紧致、更容易处理的空间，同时保持原有向量间的相对位置关系。\n",
    "- **过程**：一般涉及两个步骤：量化和编码。量化是通过聚类或其他技术将向量量化为索引中的桶，编码则是将量化后的向量进行编码以节省存储空间。\n",
    "\n",
    "4. 索引类型\n",
    "\n",
    "- **暴力搜索索引（精确）**：在CPU或GPU上使用基于余弦相似度或欧式距离的全量计算，寻找最相近的向量。\n",
    "- **逼近索引（近似）**：借助聚类或其他技术做相似度近似计算，结果通常不精确但计算上更高效。\n",
    "\n",
    "5. 安装和使用\n",
    "\n",
    "- **安装**：可以通过pip或conda进行安装，支持CPU和GPU两种模式。\n",
    "- **使用**：包括创建索引、添加向量到索引、进行相似性搜索等基本操作。Faiss还支持其他高级功能，如聚类、PQ索引等。\n",
    "\n",
    "6. 性能分析\n",
    "\n",
    "- Faiss在相似性搜索方面表现出色，主要得益于其优化的索引结构和并行计算能力。\n",
    "\n",
    "7. 应用场景\n",
    "\n",
    "- Faiss广泛应用于推荐系统、信息检索、语义搜索、计算机视觉等现实任务中。\n",
    "\n",
    "综上所述，Faiss向量数据库是一个功能强大、性能卓越的向量相似性搜索库，适用于处理大规模向量数据集并进行快速相似性搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的模块和类\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 实例化向量嵌入器\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# 初始化缓存存储器\n",
    "store = LocalFileStore(\"./faiss/\")\n",
    "# 创建缓存支持的嵌入器\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bates_store(\n",
    "    embeddings,\n",
    "    store,\n",
    "    namespace=embeddings.model,\n",
    ")\n",
    "# 加载文档并将其踹分成片段\n",
    "doc = TextLoader(\"../data/day06.md\", encoding='utf-8').load()\n",
    "spliter = CharacterTextSplitter(\"\\n\", chunk_size=200, chunk_overlap=0)\n",
    "chunks = spliter.split_documents(doc)\n",
    "# 创建向量存储\n",
    "db = FAISS.from_documents(chunks, cached_embeddings)\n",
    "db.similarity_search(\"NBA冠军球队是哪个？\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.3 Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Milvus是一个开源的向量数据库，由Zilliz公司发起并维护。它专为处理非结构化数据而设计，能够存储、检索和分析大量的向量数据。以下是关于Milvus的详细介绍：\n",
    "\n",
    "一、基本概述\n",
    "\n",
    "- **名称由来**：Milvus的名字来源于拉丁语，意为“一万”，象征着其处理大规模数据集的能力。\n",
    "- **创建时间**：Milvus创建于2019年，其唯一目标是存储、索引和管理由深度神经网络和其他机器学习(ML)模型生成的大规模嵌入向量。\n",
    "- **数据处理能力**：作为一个专门设计用于处理输入向量查询的数据库，Milvus能够处理万亿级别的向量索引。\n",
    "\n",
    "二、核心特性\n",
    "\n",
    "1. **高性能**：Milvus提供了高效的向量搜索能力，支持毫秒级的最近邻搜索，即使在亿级向量规模下也能保持高性能。\n",
    "2. **高可用、高可靠**：Milvus支持在云上扩展，其容灾能力能够保证服务高可用。同时，它采用共享存储架构，存储计算完全分离，计算节点支持横向扩展。\n",
    "3. **灵活性与兼容性**：Milvus与多种机器学习框架兼容，如TensorFlow、PyTorch和PaddlePaddle，可以轻松集成到现有的机器学习工作流程中。此外，它还提供了简单易用的API，支持多种编程语言，如Python、Java和Go。\n",
    "4. **多索引类型**：Milvus支持多种索引类型，如FLAT、IVF、HNSW等，以适应不同的搜索性能和存储效率需求。\n",
    "5. **混合查询**：Milvus支持在向量相似度检索过程中进行标量字段过滤，实现混合查询，进一步提高了召回率和搜索的灵活性。\n",
    "\n",
    "三、系统架构\n",
    "\n",
    "Milvus的系统架构分为四个层次：\n",
    "\n",
    "- **接入层(Access Layer)**：由一组无状态proxy组成，对外提供用户连接的endpoint，负责验证客户端请求并合并返回结果。\n",
    "- **协调服务(Coordinator Service)**：充当系统的大脑，负责分配任务给执行节点。协调服务共有四种角色，分别为root coord、data coord、query coord和index coord。\n",
    "- **执行节点(Worker Node)**：负责完成协调服务下发的指令和proxy发起的数据操作语言(DML)命令。执行节点分为三种角色，分别为data node、query node和index node。\n",
    "- **存储层(Storage)**：负责Milvus数据的持久化，分为元数据存储(meta store)、消息存储(log broker)和对象存储(object storage)三个部分。\n",
    "\n",
    "四、应用场景\n",
    "\n",
    "Milvus适用于需要处理大规模向量数据的场景，特别是在以下领域有广泛应用：\n",
    "\n",
    "- **机器学习**：在机器学习模型训练后，Milvus可以用来存储和搜索模型生成的向量。\n",
    "- **计算机视觉**：用于图像和视频分析中的向量搜索，如图像匹配、相似图像搜索等。\n",
    "- **语音识别**：在语音识别系统中，Milvus可以用来检索与查询语音最相似的向量。\n",
    "- **推荐系统**：在推荐系统中，Milvus可以用来找到用户可能感兴趣的商品或内容。\n",
    "- **自然语言处理**：Milvus可以用来检索与查询文本最相关的文档或句子。\n",
    "\n",
    "此外，Milvus还可以应用于音频相似性搜索、分子相似性搜索等多个领域，为这些领域的数据处理和分析提供强大的支持。\n",
    "\n",
    "综上所述，Milvus作为一款开源的向量数据库，以其高性能、高可用、高可靠和灵活性等特性，在机器学习、计算机视觉、语音识别等多个领域有着广泛的应用前景。\n",
    "\n",
    "安装使用\n",
    "\n",
    "docker安装\n",
    "\n",
    "CentOS 7 安装 Docker 的步骤如下：\n",
    "\n",
    "1. 卸载旧版本的 Docker（如果有）:\n",
    "\n",
    "```\n",
    "sudo yum remove docker \\                  docker-client \\                  docker-client-latest \\                  docker-common \\                  docker-latest \\                  docker-latest-logrotate \\                  docker-logrotate \\                  docker-engine\n",
    "```\n",
    "\n",
    "1. 安装 Docker 依赖的软件包:\n",
    "\n",
    "```\n",
    "sudo yum install -y yum-utils\n",
    "```\n",
    "\n",
    "1. 设置 Docker 仓库:\n",
    "\n",
    "```\n",
    "sudo yum-config-manager \\    --add-repo \\    https://download.docker.com/linux/centos/docker-ce.repo\n",
    "```\n",
    "\n",
    "1. 安装 Docker Engine-Community:\n",
    "\n",
    "```\n",
    "sudo yum install docker-ce docker-ce-cli containerd.io\n",
    "```\n",
    "\n",
    "1. 更新镜像\n",
    "\n",
    "   ~~~\n",
    "   sudo mkdir -p /etc/docker\n",
    "   sudo tee /etc/docker/daemon.json <<-'EOF'\n",
    "   {\n",
    "     \"registry-mirrors\": [\"https://gt8iqili.mirror.aliyuncs.com\"]\n",
    "   }\n",
    "   EOF\n",
    "   sudo systemctl daemon-reload\n",
    "   sudo systemctl restart docker\n",
    "   ~~~\n",
    "\n",
    "   \n",
    "\n",
    "2. 启动 Docker 服务:\n",
    "\n",
    "```\n",
    "sudo systemctl start docker\n",
    "```\n",
    "\n",
    "1. 验证 Docker 是否正确安装:\n",
    "\n",
    "```\n",
    "sudo docker run hello-world\n",
    "```\n",
    "\n",
    "这些命令应以 root 用户或使用 sudo 执行。每一步都需要网络连接以从 Docker 仓库下载所需的包。\n",
    "\n",
    "docker pull milvusdb/milvus:latest\n",
    "\n",
    "docker run -d --name milvus_server -p 19530:19530 -p 9091:9091 milvusdb/milvus:latest\n",
    "\n",
    "docker ps查看正在运行的镜像\n",
    "\n",
    "测试\n",
    "\n",
    "pip3 install pymilvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection, connections, DataType  \n",
    "  \n",
    "# 连接到Milvus服务器  \n",
    "connections.connect(\"default\", host=\"124.71.227.70\", port=\"19530\")  \n",
    "  \n",
    "# 创建集合  \n",
    "field_schemas = [  \n",
    "    {\"name\": \"id\", \"dtype\": DataType.INT64, \"is_primary\": True, \"auto_id\": True},  \n",
    "    {\"name\": \"embedding\", \"dtype\": DataType.FLOAT_VECTOR, \"dim\": 128}  \n",
    "]  \n",
    "collection_schema = CollectionSchema(fields=field_schemas, description=\"test collection\")  \n",
    "collection = Collection(\"test_collection\", schema=collection_schema)  \n",
    "  \n",
    "# 插入向量  \n",
    "vectors = [[random.random() for _ in range(128)] for _ in range(10)]  \n",
    "collection.insert(vectors)  \n",
    "  \n",
    "# 执行搜索  \n",
    "query_vector = [random.random() for _ in range(128)]  \n",
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}  \n",
    "results = collection.search(query_vector, anns_field=\"embedding\", params=search_params, limit=5)  \n",
    "  \n",
    "# 打印搜索结果  \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、Chains and Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 **什么是链**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在简单应用中，单独使用LLM是可以的，但更复杂的应用需要将LLM进行链接。例如，我们可以创建一个链，该链接收用户输入，使用PromptTemplate对其进行格式化，然后将格式化后的响应传递给LLM。链允许我们将多个组件组合在一起创建一个单一的、连贯的应用。\n",
    "\n",
    "四个常用的链：\n",
    "\n",
    "\\- LLMChain：一个链，将一个LLM和一个PromptTemplate组合在一起。\n",
    "\n",
    "\\- SimpleSequentialChain：一个简单的链，将一个链的输出作为下一个链的输入。\n",
    "\n",
    "\\- SequentialChain：一个更复杂的链，允许我们定义多个链，并将它们链接在一起。\n",
    "\n",
    "\\- ConversationChain：一个链，将一个LLM和一个ConversationPromptTemplate组合在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8.2 LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 LLMChain介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMChain是一个在语言模型周围添加功能的简单链，它被广泛地应用于LangChain中，包括其他链和代理。以下是关于LLMChain的详细解释：\n",
    "\n",
    "1. 定义与结构\n",
    "   - LLMChain由一个PromptTemplate和一个语言模型（LLM或聊天模型）组成。\n",
    "   - 它接受用户输入，使用PromptTemplate进行格式化，然后将格式化后的响应传递给LLM。\n",
    "2. 应用背景\n",
    "   - 在LangChain这样的框架中，LLMChain被用作构建基于大语言模型（LLM）应用程序的重要组件。\n",
    "   - LangChain旨在简化LLM应用程序的开发过程，并提供了一系列模块和工具来支持常见的用例。\n",
    "3. 使用场景\n",
    "   - LLMChain的使用场景广泛，包括但不限于聊天机器人、智能问答工具、文档分析和摘要、代码分析等。\n",
    "   - 开发人员可以使用LLMChain来构建上下文感知、推理应用程序，实现从原型到生产环境的快速转化。\n",
    "4. 优势与特点\n",
    "   - 模块化设计：LLMChain作为LangChain框架的一部分，采用模块化设计，使得开发者能够更加便捷地进行模块化的开发和维护。\n",
    "   - 易用性强：LangChain框架提供了丰富的API和文档，让开发者可以更加便捷地进行LLM应用的开发。\n",
    "   - 高性能：LangChain框架在底层进行了优化，保证了LLM应用的高性能，使得开发者可以在保证性能的前提下，更加专注于业务逻辑的实现。\n",
    "5. 工作原理\n",
    "   - 当用户输入一个提示时，LLMChain首先使用PromptTemplate对输入进行格式化。\n",
    "   - 然后，它将格式化后的提示传递给LLM（如ChatGPT），由LLM生成相应的回答或响应。\n",
    "   - 最后，LLMChain将LLM生成的响应返回给用户。\n",
    "6. 集成与合作\n",
    "   - LLMChain可以与其他LangChain组件（如Agents、Chains、Memory等）进行集成，以构建更复杂、更强大的LLM应用程序。\n",
    "   - 它还可以与各种LLM和聊天模型进行配合，如OpenAI的GPT系列模型，以实现更丰富的功能和更高的性能。\n",
    "7. 发展趋势\n",
    "   - 随着人工智能技术的不断发展，LLMChain和LangChain等框架将继续在LLM应用开发领域发挥重要作用。\n",
    "   - 未来，这些框架可能会进一步优化性能、提升易用性，并扩展更多的应用场景和功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 LLMChain案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "\n",
    "llm = Tongyi()\n",
    "promptTemplate = PromptTemplate.from_template(\"你是起名大师，我家是{sex}宝，姓{firstName}，请起3个好养的名字？\")\n",
    "\n",
    "# 多个参数\n",
    "chain = LLMChain(llm=llm, prompt=promptTemplate, verbose=True)\n",
    "ret = chain.invoke({'sex': \"男\",'firstname': \"翟\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.1 SimpleSequentialChain介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只支持固定的链路\n",
    "\n",
    "SimpleSequentialChain是Langchain框架中的一种顺序链类型，它的主要特点是每个步骤都具有单一输入/输出，并且一个步骤的输出是下一个步骤的输入。以下是关于SimpleSequentialChain的详细解释：\n",
    "\n",
    "1. 定义与结构\n",
    "   - SimpleSequentialChain是顺序链的最简单形式，它允许用户将多个单输入/单输出链连接成一个链。\n",
    "   - 在这种链中，每个步骤的输出都会直接传递给下一个步骤作为输入。\n",
    "2. 特点\n",
    "   - **单一输入输出**：每个步骤都接受一个输入并产生一个输出，这个输出将作为下一个步骤的输入。\n",
    "   - **顺序执行**：步骤按照定义的顺序依次执行，确保数据按照预期的方式流动。\n",
    "   - **易于理解**：由于其简单性，SimpleSequentialChain易于理解和实现，特别适用于简单的场景。\n",
    "3. 使用场景\n",
    "   - SimpleSequentialChain适用于那些需要按顺序执行多个单输入/单输出步骤的场景。\n",
    "   - 例如，在一个文本处理任务中，可能需要先对文本进行分词，然后对每个词进行词性标注，最后进行命名实体识别。这些步骤可以作为一个SimpleSequentialChain来执行。\n",
    "4. 与SequentialChain的区别\n",
    "   - SequentialChain是更通用的顺序链形式，它允许多个输入/输出，并且步骤之间可以有更复杂的依赖关系。\n",
    "   - 相比之下，SimpleSequentialChain更加简单和直接，适用于那些不需要复杂依赖关系的场景。\n",
    "5. 示例\n",
    "   - 假设我们有一个简单的文本处理任务，需要先将文本转换为小写，然后去除标点符号，最后进行分词。这三个步骤可以作为一个SimpleSequentialChain来执行。\n",
    "   - 在这个示例中，第一个步骤的输入是原始文本，输出是转换为小写的文本；第二个步骤的输入是转换为小写的文本，输出是去除标点符号的文本；第三个步骤的输入是去除标点符号的文本，输出是分词后的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "llm = Tongyi()\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "\n",
    "#chain 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\"帮我给{product}的公司起一个响亮容易记忆的名字?\")\n",
    "chain_one = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt\n",
    ")\n",
    "\n",
    "#chain 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\"用5个词来描述一下这个公司名字：{company_name}\")\n",
    "chain_two = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt\n",
    ")\n",
    "# 实例化\n",
    "simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True,#打开日志\n",
    ")\n",
    "simple_chain.invoke(\"动漫制作\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 SequentialChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持多个链路的顺序执行\n",
    "\n",
    "SequentialChain是LangChain库中的一个重要概念，它允许用户将多个链（Chain）按照特定的顺序连接起来，形成一个处理流程。以下是关于SequentialChain的详细解释：\n",
    "\n",
    "定义与结构\n",
    "\n",
    "- **SequentialChain**：它是一种链式结构，可以将多个LLMChain（或其他类型的Chain）按照特定的顺序连接起来。这种结构使得一个大任务可以被分解为多个小任务，并依次执行。\n",
    "- **结构特点**：SequentialChain允许用户定义任务的执行顺序，并且每个任务的输出可以作为下一个任务的输入。这种机制非常适合处理具有依赖关系的任务序列。\n",
    "\n",
    "特点\n",
    "\n",
    "1. **顺序执行**：SequentialChain中的任务按照用户定义的顺序依次执行，确保数据按照预期的方式流动。\n",
    "2. **灵活组合**：用户可以根据需要自由组合不同的Chain，形成复杂的处理流程。\n",
    "3. **可扩展性**：SequentialChain的设计允许用户轻松地添加新的任务或修改现有任务的顺序。\n",
    "4. **数据传递**：SequentialChain支持任务之间的数据传递，即一个任务的输出可以作为下一个任务的输入。\n",
    "\n",
    "类型\n",
    "\n",
    "- **SimpleSequentialChain**：这是SequentialChain的最简单形式，其中每个步骤都具有单一输入/输出，并且一个步骤的输出是下一个步骤的输入。\n",
    "- **SequentialChain**：更通用形式的顺序链，允许多个输入/输出，可以处理更复杂的场景。\n",
    "\n",
    "使用场景\n",
    "\n",
    "- **文本处理**：例如，将一个长文本首先进行文本清洗，然后进行情感分析，最后生成摘要。这三个任务可以按照顺序连接成一个SequentialChain。\n",
    "- **问答系统**：在处理复杂问题时，可以将问题解析、信息检索和答案生成等任务连接成一个SequentialChain，实现自动化的问答功能。\n",
    "\n",
    "示例\n",
    "\n",
    "假设我们有一个任务是将用户输入的文本转换为小写，然后统计其中的单词数。这个任务可以分解为两个子任务：文本转小写和单词计数。这两个子任务可以连接成一个SequentialChain：\n",
    "\n",
    "1. **文本转小写Chain**：接收用户输入的文本作为输入，输出转换为小写后的文本。\n",
    "2. **单词计数Chain**：接收小写文本作为输入，输出文本中的单词数。\n",
    "\n",
    "通过将这两个Chain按照顺序连接起来，我们就可以形成一个完整的SequentialChain来处理用户的输入文本了。\n",
    "\n",
    "总之，SequentialChain是LangChain库中一个非常强大的工具，它允许用户将多个任务连接成一个处理流程，并按照特定的顺序执行这些任务。这种机制可以大大提高任务的执行效率和准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.sequential import SequentialChain\n",
    "\n",
    "#chain 1 任务：翻译成中文\n",
    "first_prompt = ChatPromptTemplate.from_template(\"把下面内容翻译成中文:\\n\\n{content}\")\n",
    "chain_one = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt,\n",
    "    # verbose=True,\n",
    "    output_key=\"Chinese_Rview\",\n",
    ")\n",
    "#chain 2 任务：对翻译后的中文进行总结摘要 input_key是上一个chain的output_key\n",
    "second_prompt = ChatPromptTemplate.from_template(\"用一句话总结下面内容:\\n\\n{Chinese_Rview}\")\n",
    "chain_two = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt,\n",
    "    # verbose=True,\n",
    "    output_key=\"Chinese_Summary\",\n",
    ")\n",
    "#chain 3 任务:智能识别语言 input_key是上一个chain的output_key\n",
    "third_prompt = ChatPromptTemplate.from_template(\"根据下面内容写5条评价信息:\\n\\n{Chinese_Summary}\")\n",
    "chain_three = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=third_prompt,\n",
    "    # verbose=True,\n",
    "    output_key=\"Language\",\n",
    ")\n",
    "#chain 4 任务:针对摘要使用指定语言进行评论 input_key是上一个chain的output_key   \n",
    "fourth_prompt = ChatPromptTemplate.from_template(\"请使用指定的语言对以下内容进行回复:\\n\\n内容:{Chinese_Summary}\\n\\n语言:{Language}\")\n",
    "chain_four = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=fourth_prompt,\n",
    "    verbose=True,\n",
    "    output_key=\"Reply\",\n",
    ")\n",
    "#overall 任务：翻译成中文->对翻译后的中文进行总结摘要->智能识别语言->针对摘要使用指定语言进行评论\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    verbose=True,\n",
    "    input_variables=[\"content\"],\n",
    "    output_variables=[\"Chinese_Rview\", \"Chinese_Summary\", \"Language\"],\n",
    ")\n",
    "content = \"I am a student of Cumulus Education, my course is artificial intelligence, I like this course, because I can get a high salary after graduation\"\n",
    "ret = overall_chain.invoke(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 RouterChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RouterChain 是一个特殊的 Chain，它可以根据输入动态选择下一个 Chain。\n",
    "\n",
    "RouterChain 由两个组件组成:\n",
    "\n",
    "RouterChain (负责选择下一个要调用的链条)\n",
    "\n",
    "destination_chains: 路由器链条可以路由到的链条\n",
    "\n",
    "RouterChain 包括不同类型的路由链条。下面的例子是 MultiPromptChain 中的使用方式，以创建一个问答链条，该链条根据给定的问题选择最相关的提示，并使用该提示回答问题。\n",
    "\n",
    "RouterChain在LangChain库中扮演着重要的角色，它主要用于实现基于条件判断的路由功能，使得智能体或应用能够根据输入内容动态地选择不同的处理流程。以下是关于RouterChain的详细解释：\n",
    "\n",
    "定义\n",
    "\n",
    "RouterChain，也称为分支链，它能够根据输入内容动态地选择并调用下一个链。在智能体的可视化编排流程中，RouterChain扮演着决策中心的角色，通过灵活的意图配置和链连接点，为智能体提供了强大的决策能力。\n",
    "\n",
    "特点\n",
    "\n",
    "1. **条件判断**：RouterChain的核心功能是根据输入的提示词或指令进行条件判断，以确定下一步应该调用哪个链。\n",
    "2. **动态选择**：基于判断结果，RouterChain能够动态地选择一个或多个目标链进行调用，从而实现不同的处理流程。\n",
    "3. **意图配置**：RouterChain允许用户定制决策逻辑，包括传入对话历史记录、输出关键词等配置项，以满足不同的应用场景需求。\n",
    "4. **链连接点**：RouterChain具有链输入连接点和链输出连接点，这些连接点是其决策功能的实现基础。链输入连接点允许RouterChain与其他任意链相连，接收输入信息；链输出连接点需要配置意图，每个意图对应一个链输出连接点。\n",
    "\n",
    "工作原理\n",
    "\n",
    "1. **输入接收**：RouterChain接收来自用户的输入，该输入可以是文本、语音或其他形式的数据。\n",
    "2. **条件判断**：根据用户输入的内容，RouterChain会调用内置的AI模型进行条件判断。这些判断可以基于关键词匹配、语义理解等多种算法实现。\n",
    "3. **目标链选择**：基于判断结果，RouterChain会选择一个或多个目标链进行调用。这些目标链可以是预先定义好的处理流程，也可以是用户自定义的链。\n",
    "4. **输出传递**：RouterChain将用户输入传递给选定的目标链，并等待目标链的处理结果。一旦目标链处理完成，RouterChain会将结果返回给用户或传递给下一个处理环节。\n",
    "\n",
    "应用场景\n",
    "\n",
    "RouterChain在多个领域都有广泛的应用，包括但不限于：\n",
    "\n",
    "- **智能客服**：在智能客服系统中，RouterChain可以根据用户的问题类型选择不同的回复模板或处理流程，提高客服效率和用户体验。\n",
    "- **推荐系统**：在推荐系统中，RouterChain可以根据用户的兴趣和行为数据选择不同的推荐算法或模型，实现更精准的个性化推荐。\n",
    "- **自然语言处理**：在自然语言处理领域，RouterChain可以根据文本内容选择不同的分析器或处理流程，实现更复杂的文本理解和处理任务。\n",
    "\n",
    "总结\n",
    "\n",
    "RouterChain是LangChain库中一个重要的组件，它通过条件判断和动态选择机制实现了智能体的决策功能。在实际应用中，RouterChain可以根据不同的输入内容和应用场景灵活配置决策逻辑和链连接点，为智能体提供了强大的决策能力和适应性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "#物理链\n",
    "physics_template = \"\"\"您是一位非常聪明的物理教授.\\n\n",
    "您擅长以简洁易懂的方式回答物理问题.\\n\n",
    "当您不知道问题答案的时候，您会坦率承认不知道.\\n\n",
    "下面是一个问题:\n",
    "{input}\"\"\"\n",
    "physics_prompt = PromptTemplate.from_template(physics_template)\n",
    "# 物理的任务\n",
    "physicschain = LLMChain( llm=llm,prompt=physics_prompt,verbose=True)\n",
    "#数学链\n",
    "math_template = \"\"\"您是一位非常优秀的数学教授.\\n\n",
    "您擅长回答数学问题.\\n\n",
    "您之所以如此优秀，是因为您能够将困难问题分解成组成的部分，回答这些部分，然后将它们组合起来，回答更广泛的问题.\\n\n",
    "下面是一个问题:\n",
    "{input}\"\"\"\n",
    "math_prompt = PromptTemplate.from_template(math_template)\n",
    "###数学的任务\n",
    "mathschain = LLMChain(llm=llm, prompt=math_prompt, verbose=True)\n",
    "# 默认任务\n",
    "default_chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    output_key=\"text\"\n",
    ")\n",
    "# 组合路由任务\n",
    "destination_chains = {}  \n",
    "destination_chains[\"physics\"] = physicschain\n",
    "destination_chains[\"math\"] = mathschain\n",
    "# 定义路由Chain\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=\"physics:擅长回答物理问题\\n math:擅长回答数学问题\")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm,\n",
    "    router_prompt\n",
    ")\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")\n",
    "# question = \"什么是牛顿第一定律?\"\n",
    "# print(router_chain.invoke(question))\n",
    "# chain.run(question)\n",
    "# question = \"2+2等于几?\"\n",
    "# print(router_chain.invoke(question))\n",
    "# print(chain.run(\"2+2等于几?\"))\n",
    "question = \"你是谁?\"\n",
    "# print(router_chain.invoke(question))\n",
    "chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 TransformChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6.1介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通用的转换链。下面的例子是创建一个虚拟转换，它接收一个超长的文本，将文本过滤为仅保留前三个段落，然后将其传递给 LLMChain 进行摘要生成。\n",
    "\n",
    "TransformChain在LangChain库中是一个重要的组件，主要用于处理chains之间的输入和输出数据，以便于chains之间的数据传输。以下是关于TransformChain的详细解释：\n",
    "\n",
    "定义\n",
    "\n",
    "TransformChain是一个框架或工具，它允许用户定义自定义的转换函数，这些函数可以应用于chains之间的数据。这意味着，当数据从一个chain传递到另一个chain时，TransformChain可以提供一个或多个转换步骤，以修改或格式化数据，使其符合下一个chain的输入要求。\n",
    "\n",
    "特点\n",
    "\n",
    "1. **自定义转换**：TransformChain支持用户定义自己的转换函数，这些函数可以根据具体需求对输入数据进行处理。\n",
    "2. **数据处理**：TransformChain的主要作用是对chains之间的数据进行处理，确保数据在传递过程中满足下一个chain的输入要求。\n",
    "3. **灵活性**：由于支持自定义转换函数，TransformChain具有很高的灵活性，可以适应各种复杂的数据处理场景。\n",
    "\n",
    "工作原理\n",
    "\n",
    "- **数据接收**：TransformChain接收来自上一个chain的输出数据。\n",
    "- **数据转换**：TransformChain应用用户定义的转换函数对接收到的数据进行处理。这些转换函数可以是简单的数据清洗、格式化，也可以是复杂的逻辑处理。\n",
    "- **数据输出**：经过转换的数据被传递给下一个chain作为输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6.2案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "\n",
    "# 第一个任务\n",
    "def transform_func(inputs:dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    shortened_text = \"\\n\".join(text.split(\"\\n\")[:3])\n",
    "    return {\"output_text\":shortened_text}\n",
    "#文档转换链\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"output_text\"],\n",
    "    transform=transform_func\n",
    ")\n",
    "# 第二个任务\n",
    "template = \"\"\"对下面的文字进行总结:\n",
    "{output_text}\n",
    "总结:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"output_text\"],\n",
    "    template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "#使用顺序链连接起来\n",
    "squential_chain = SimpleSequentialChain(\n",
    "    chains=[transform_chain,llm_chain],\n",
    "    verbose=True\n",
    ")\n",
    "with open(\"NBA/NBA新闻.txt\",encoding='utf-8') as f:\n",
    "    letters = f.read()\n",
    "squential_chain.invoke(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 四种文档处理链\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  8.7.1 StuffDocumentsChain 整合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这种链最简单直接，是将所有获取到的文档作为 context 放入到 Prompt 中，传递到 LLM 获取答案。这种方式可以完整的保留上下文，调用 LLM 的次数也比较少，建议能使用 stuff 的就使用这种方式。其适合文档拆分的比较小，一次获取文档比较少的场景，不然容易超过 token 的限制。\n",
    "\n",
    "StuffDocumentsChain在LangChain框架中是一个用于处理文档的关键组件，它主要用于将多个文档组装成一个提示（prompt），并将这个提示传递给大模型（LLM）。以下是关于StuffDocumentsChain的详细解释：\n",
    "\n",
    "1. 作用\n",
    "   - StuffDocumentsChain的主要作用是将多个文档的内容整合到一个单一的提示中，然后将其传递给大模型。这样做可以确保需要的信息都被传递，进而从大模型中获取所需的上下文和响应。\n",
    "2. 使用场景\n",
    "   - 当需要处理多个文档，并且这些文档的内容需要一起作为输入传递给大模型时，可以使用StuffDocumentsChain。例如，在处理一个包含多篇相关文章的文档集合，并希望将这些文章的内容作为一个整体输入给大模型进行摘要、问答等任务时。\n",
    "3. 优点\n",
    "   - 简单易用：StuffDocumentsChain提供了一种简单的方式，将多个文档的内容合并成一个提示，从而简化了与大模型的交互。\n",
    "   - 保持上下文完整性：通过将多个文档的内容整合到一个提示中，可以确保大模型在处理时能够考虑到这些文档之间的上下文关系。\n",
    "4. 注意事项\n",
    "   - 当文档的数量或长度非常大时，可能会超出大模型的上下文窗口限制，导致信息丢失或模型性能下降。因此，在使用StuffDocumentsChain时，需要注意文档的数量和长度，并考虑是否需要进行适当的分割或简化。\n",
    "5. 与其他组件的协同\n",
    "   - StuffDocumentsChain通常与其他LangChain组件一起使用，如LLM（大模型）加载器、文档加载器等，以构建一个完整的文档处理流程。例如，可以使用文档加载器从外部数据源加载文档，然后使用StuffDocumentsChain将文档整合成提示，最后传递给大模型进行处理。\n",
    "\n",
    "综上所述，StuffDocumentsChain是LangChain框架中一个重要的文档处理组件，它通过将多个文档的内容整合到一个提示中，并传递给大模型，从而简化了与大模型的交互，并保持了上下文的完整性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 1 定义prompt模板\n",
    "prompt_template = \"\"\"对以下文字做简洁的总结:\n",
    "{text}\n",
    "简洁的总结:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# 2 定义任务\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 3、定义chain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"text\",\n",
    ")\n",
    "\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# split = CharacterTextSplitter(\"\\n\",chunk_size = 200)\n",
    "# text = split.split_documents(docs)\n",
    "\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.2 RefineDocumentsChain 递归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过迭代更新的方式获取答案。先处理第一个文档，作为 context 传递给 llm，获取中间结果 intermediate answer。然后将第一个文档的中间结果以及第二个文档发给 llm 进行处理，后续的文档类似处理。Refine 这种方式能部分保留上下文，以及 token 的使用能控制在一定范围。\n",
    "\n",
    "RefineDocumentsChain是LangChain框架中的一个重要组件，它用于顺序地处理多个文档，并通过迭代的方式改进生成的答案。以下是关于RefineDocumentsChain的详细解释：\n",
    "\n",
    "1. 基本概念\n",
    "\n",
    "- **作用**：RefineDocumentsChain的主要作用是基于第一个文档生成初始答案，然后循环处理其余文档以改进该答案。它鼓励在多个文档之间进行信息传递，以产生更准确和完整的回答。\n",
    "- **使用场景**：当需要处理多个文档，并且希望生成的答案能够综合所有文档的信息时，可以使用RefineDocumentsChain。例如，在问答系统中，当用户的问题涉及多个文档时，可以使用RefineDocumentsChain来生成一个综合所有文档信息的答案。\n",
    "\n",
    "2. 工作原理\n",
    "\n",
    "- **顺序处理**：RefineDocumentsChain按照顺序处理文档，首先基于第一个文档生成初始答案。\n",
    "- **迭代改进**：然后，它循环处理其余文档，每次使用当前文档的内容和之前生成的答案作为上下文，生成一个新的答案。这个过程不断迭代，直到处理完所有文档。\n",
    "- **信息传递**：RefineDocumentsChain鼓励在多个文档之间进行信息传递。这意味着，在处理一个文档时，它会考虑之前文档的内容和处理结果，以确保生成的答案能够综合所有文档的信息。\n",
    "\n",
    "3. 优点\n",
    "\n",
    "- **准确性**：由于RefineDocumentsChain能够综合多个文档的信息，因此生成的答案通常更准确和全面。\n",
    "- **可解释性**：由于RefineDocumentsChain的处理过程是顺序和可迭代的，因此可以更容易地理解和解释生成的答案是如何从多个文档中得出的。\n",
    "\n",
    "4. 注意事项\n",
    "\n",
    "- **文档数量**：虽然RefineDocumentsChain可以处理多个文档，但当文档数量非常大时，处理时间可能会增加。因此，在实际应用中需要根据具体情况考虑是否需要对文档进行筛选或合并。\n",
    "- **上下文窗口限制**：与所有基于大模型的方法一样，RefineDocumentsChain也受到大模型上下文窗口的限制。如果文档的总长度超过了模型的上下文窗口大小，可能需要考虑对文档进行分割或选择更合适的方法来处理。\n",
    "\n",
    "5. 示例\n",
    "\n",
    "假设有一个问答系统，用户的问题涉及三个文档A、B和C。使用RefineDocumentsChain处理这些文档时，首先基于文档A生成初始答案。然后，在处理文档B时，会将文档B的内容和之前生成的答案作为上下文，生成一个新的答案。最后，在处理文档C时，同样会将文档C的内容和之前生成的答案作为上下文，生成最终的答案。这样，最终的答案就能够综合文档A、B和C的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载问答\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=400, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "prompt_template = \"\"\"对以下文字做简洁的总结:\n",
    "{text}\n",
    "简洁的总结:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"你的任务是产生最终摘要\\n\"\n",
    "    \"我们已经提供了一个到某个特定点的现有回答:{existing_answer}\\n\"\n",
    "    \"我们有机会通过下面的一些更多上下文来完善现有的回答(仅在需要时使用).\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"根据新的上下文，用中文完善原始回答.\\n\"\n",
    "    \"如果上下文没有用处，返回原始回答。\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt = refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key = \"documents\",\n",
    "    output_key = \"output_text\",\n",
    ")\n",
    "result = chain.invoke({\"documents\":split_docs})\n",
    "print(result)\n",
    "print(result[\"output_text\"])\n",
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.3 MapReduceDocumentsChain 汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "先通过 LLM 对每个 document 进行处理，然后将所有文档的答案在通过 LLM 进行合并处理，得到最终的结果。\n",
    "\n",
    "MapReduce 的方式将每个 document 单独处理，可以并发进行调用。但是每个文档之间缺少上下文。\n",
    "\n",
    "MapReduceDocumentsChain是LangChain框架中的一个组件，它结合了MapReduce的思想来处理文档数据。以下是关于MapReduceDocumentsChain的详细解释：\n",
    "\n",
    "1. 基本概念\n",
    "\n",
    "- **作用**：MapReduceDocumentsChain主要用于处理大规模的文档数据。它首先通过Map操作将每个文档传递给大语言模型（LLM），然后利用Reduce操作对处理后的文档进行简化和聚合。\n",
    "- **使用场景**：当需要处理大量文档，并且文档之间可以并行处理时，MapReduceDocumentsChain是一个有效的选择。它适用于文档数量众多，且可以分割成多个独立部分进行并行计算的情况。\n",
    "\n",
    "2. 工作原理\n",
    "\n",
    "- Map阶段\n",
    "\n",
    "  ：\n",
    "\n",
    "  - 将输入的文档集合分割成多个块（blocks），每个块可以并行处理。\n",
    "  - 将每个文档块传递给LLM进行处理，提取关键信息或生成初步结果。\n",
    "\n",
    "- Reduce阶段\n",
    "\n",
    "  ：\n",
    "\n",
    "  - 收集Map阶段生成的所有初步结果。\n",
    "  - 使用ReduceDocumentsChain或其他组件对初步结果进行简化、聚合或进一步处理。\n",
    "  - 最终生成一个综合了所有文档信息的输出。\n",
    "\n",
    "3. 优点\n",
    "\n",
    "- **并行处理**：MapReduceDocumentsChain利用MapReduce的并行处理特性，可以显著提高处理大规模文档数据的效率。\n",
    "- **简化文档**：通过ReduceDocumentsChain的简化操作，可以将复杂的文档数据简化为更易于理解和处理的格式。\n",
    "- **灵活性**：MapReduceDocumentsChain可以与其他LangChain组件结合使用，构建出更复杂、更强大的文档处理流程。\n",
    "\n",
    "4. 注意事项\n",
    "\n",
    "- **文档数量**：虽然MapReduceDocumentsChain适用于处理大量文档，但当文档数量过于庞大时，可能需要考虑更高效的分割和并行处理策略。\n",
    "- **上下文窗口限制**：由于LLM的上下文窗口限制，对于过长的文档或文档集合，可能需要采取分块处理或其他策略来避免信息丢失。\n",
    "\n",
    "5. 与其他组件的协同\n",
    "\n",
    "- **LLM**：MapReduceDocumentsChain依赖于LLM进行文档处理。LLM的选择和配置将直接影响MapReduceDocumentsChain的性能和效果。\n",
    "- **ReduceDocumentsChain**：MapReduceDocumentsChain的Reduce阶段通常使用ReduceDocumentsChain或其他组件来简化和聚合初步结果。这些组件的选择和配置将影响最终输出的质量和准确性。\n",
    "\n",
    "总之，MapReduceDocumentsChain是一个强大的文档处理组件，它结合了MapReduce的并行处理特性和LangChain的文档处理能力，适用于处理大规模、复杂的文档数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载问答\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "loader = TextLoader(\"NBA/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "#split\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=400, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "prompt_template = \"\"\"对以下文字做简洁的总结:\n",
    "{text}\n",
    "简洁的总结:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "refine_template = (\n",
    "    \"你的任务是产生最终摘要\\n\"\n",
    "    \"我们已经提供了一个到某个特定点的现有回答:{existing_answer}\\n\"\n",
    "    \"我们有机会通过下面的一些更多上下文来完善现有的回答(仅在需要时使用).\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"根据新的上下文，用中文完善原始回答.\\n\"\n",
    "    \"如果上下文没有用处，返回原始回答。\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt = refine_prompt,\n",
    "    # return_intermediate_steps=True,\n",
    "    input_key = \"documents\",\n",
    "    output_key = \"output_text\",\n",
    "    verbose=True\n",
    ")\n",
    "result = chain.invoke({\"documents\":split_docs})\n",
    "print(result)\n",
    "# print(result[\"output_text\"])\n",
    "# print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.4 MapRerankDocumentsChain 选分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapRerankDocumentsChain 和 MapReduceDocumentsChain 类似，先通过 LLM 对每个 document 进行处理，每个答案都会返回一个 score，最后选择 score 最高的答案。MapRerank 和 MapReduce 类似，会大批量的调用 LLM，每个 document 之间是独立处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.reduce import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "#load\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "#split\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=200, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_rerank\", \n",
    "    metadata_keys=['source'], \n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "print(chain)\n",
    "query = \"中文回答这篇文章的主要内容是什么？\"\n",
    "result = chain.invoke({\"input_documents\":split_docs,\"question\":query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7.5 自定义链"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当通用链不满足的时候，可以自行构建来实现特定的目的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.chains.base import  Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.base_language import  BaseLanguageModel\n",
    "\n",
    "class wiki_article_chain(Chain):\n",
    "    \"\"\"开发一个wiki文章生成器\"\"\"\n",
    "    prompt:BasePromptTemplate\n",
    "    llm:BaseLanguageModel\n",
    "    out_key:str=\"text\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"将返回Prompt所需的所有键\"\"\"\n",
    "        return self.prompt.input_variables\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"将始终返回text键\"\"\"\n",
    "        return [self.out_key]\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        inputs:Dict[str,Any],\n",
    "        run_manager:Optional[CallbackManagerForChainRun]=None,\n",
    "    ) -> Dict[str,Any]:\n",
    "        \"\"\"运行链\"\"\"\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        #print(\"prompt_value:\",prompt_value)\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value],callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        #print(\"response:\",response)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\"wiki article is written\")\n",
    "        return {self.out_key:response.generations[0][0].text}\n",
    "    \n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        \"\"\"链类型\"\"\"\n",
    "        return \"wiki_article_chain\"\n",
    "        \n",
    "        \n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.prompts import  PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"写一篇关于{topic}的维基百科形式的文章\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "chain = wiki_article_chain(\n",
    "    llm = llm,\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 8.8.1 memory介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "默认情况下，Chains（链）和Agents（代理）是无状态的，这意味着它们将每个传入的查询视为独立的（底层的LLM和聊天模型也是如此）。在某些应用程序中（聊天机器人就是一个很好的例子），记住先前的交互非常重要，无论是短期还是长期。Memory（记忆）正是为此而设计的。 LangChain提供两种形式的记忆组件。首先，LangChain提供了用于管理和操作先前聊天消息的辅助工具。无论如何使用，这些工具都被设计为模块化和有用。其次，LangChain提供了将这些工具轻松整合到链中的方法。\n",
    "\n",
    "通常情况下，对于每种类型的记忆，有两种理解使用记忆的方法。一种是独立的函数，从一系列消息中提取信息，另一种是在链中使用这种类型的记忆的方法。\n",
    "\n",
    "记忆可以返回多个信息（例如，最近的 N 条消息和所有先前消息的摘要）。返回的信息可以是字符串或消息列表。\n",
    "\n",
    "ChatMessageHistory\n",
    "\n",
    "在大多数（如果不是全部）记忆模块的核心实用类之一是 `ChatMessageHistory` 类。这是一个超轻量级的包装器，提供了保存人类消息、AI 消息以及获取所有消息的便捷方法。\n",
    "\n",
    "如果您在链外管理记忆，则可以直接使用此类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"你好\")\n",
    "history.add_ai_message(\"你好?\")\n",
    "history.add_user_message(\"请问丹麦的首都是哪里?\")\n",
    "history.add_ai_message(\"哥本哈根\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory\n",
    "\n",
    "现在我们展示如何在链中使用这个简单的概念。首先展示 `ConversationBufferMemory`，它只是一个对 ChatMessageHistory 的包装器，用于提取消息到一个变量中。\n",
    "\n",
    "我们可以首先将其提取为一个字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们还可以将历史记录作为消息列表获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在链中使用\n",
    "\n",
    "最后，让我们看看如何在链中使用这个模块（设置 `verbose=True` 以便查看提示）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"你好，我是小红\")\n",
    "memory.chat_memory.add_ai_message(\"小红您好，我是一个医生，可以帮助你吗\")\n",
    "memory.chat_memory.add_user_message(\"我感觉头晕，眼花，不想吃东西怎么办\")\n",
    "# memory.chat_memory.add_ai_message(\"小红您好，我是一个医生，可以帮助你吗\")\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "llm = Tongyi(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    # verbose=True, \n",
    "    memory=memory\n",
    ")\n",
    "conversation.predict(input=\"我感觉头晕，眼花，不想吃东西怎么办\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "保存消息记录\n",
    "\n",
    "您可能经常需要保存消息，并在以后使用时加载它们。可以通过将消息首先转换为普通的 Python 字典来轻松实现此操作，然后将其保存（例如，保存为 JSON 格式），然后再加载。以下是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")\n",
    "dicts = messages_to_dict(history.messages)\n",
    "\n",
    "new_messages = messages_from_dict(dicts)\n",
    "new_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "长时记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存到向量数据库(向量数据库存储Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    {\"input\":\"帮我找一下tomie\"},\n",
    "    {\"output\":\"对不起请问什么是tomie？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"tomie是一个培训讲师\"},\n",
    "    {\"output\":\"好的，我知道了。\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"今天他要讲一门关于RAG的课程\"},\n",
    "    {\"output\":\"好的，我知道了。需要RAG的资料吗？\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\":\"不需要资料了，谢谢\"},\n",
    "    {\"output\":\"好的，那我就不打扰你了。\"}\n",
    ")\n",
    "vectorstore = FAISS.from_texts(\n",
    "    memory.buffer.split(\"\\n\"),\n",
    "    DashScopeEmbeddings()\n",
    ")\n",
    "FAISS.save_local(vectorstore,\"test_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "faiss = FAISS.load_local(\"test_faiss\", DashScopeEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.similarity_search(\"tomie是什么职业\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **五、agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 agents介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents 是一个具有智能功能的智能体，它使用 LLM 和工具来执行任务。\n",
    "\n",
    "Agents 核心思想是使用LLM来选择要采取的一系列动作。在链式结构中，一系列动作是硬编码的（在代码中）。 在 Agents 中，使用语言模型作为推理引擎来确定要采取的动作及其顺序。\n",
    "\n",
    "Agents 包括几个关键组件：\n",
    "\n",
    "\\- ***\\*Agent\\****: 用于生成指令和执行动作的代理。\n",
    "\n",
    "\\- ***\\*Tool\\****: 用于执行动作的函数。\n",
    "\n",
    "\\- ***\\*Memory\\****: 用于存储历史对话和生成的指令。\n",
    "\n",
    "\\- ***\\*LLM\\****: 用于生成指令和执行动作的 LLM。\n",
    "\n",
    "有些应用程序不仅需要预先确定的LLM（语言模型）/其他工具的调用链，还可能需要依赖用户输入的未知链条。在这些类型的链条中，有一个代理程序可以访问一套工具。根据用户的输入，代理程序可以决定是否调用这些工具中的任何一个。\n",
    "\n",
    "目前，有两种主要类型的代理程序：\n",
    "\n",
    "动作代理：这些代理程序决定要采取的动作，并逐个执行这些动作。\n",
    "\n",
    "计划和执行代理：这些代理程序首先制定一套要采取的行动计划，然后逐个执行这些行动。\n",
    "\n",
    "何时使用每种类型的代理程序？动作代理更为传统，适用于小型任务。对于更复杂或长时间运行的任务，初始的规划步骤有助于保持长期目标和专注。然而，这样做通常会增加调用次数和延迟。这两种代理程序也不是互斥的 - 实际上，通常最好由动作代理负责计划和执行代理的执行。\n",
    "\n",
    "动作代理的高级伪代码如下：\n",
    "\n",
    "1. 接收用户输入。\n",
    "2. 代理程序决定是否使用某个工具，以及工具的输入应该是什么。\n",
    "3. 使用该工具以工具输入进行调用，并记录观察结果（调用的输出）。\n",
    "4. 将工具、工具输入和观察结果的历史传递回代理程序，并由代理程序决定下一步操作。\n",
    "5. 重复上述步骤，直到代理程序决定不再需要使用工具，然后直接向用户做出响应。\n",
    "\n",
    "代理程序涉及的不同抽象概念如下：\n",
    "\n",
    "- 代理程序（Agent）：这是应用程序的逻辑所在。代理程序提供一个接口，接受用户输入以及代理程序已经执行的步骤列表，并返回代理动作（AgentAction）或代理结束（AgentFinish）。\n",
    "  - 代理动作（AgentAction）：对应要使用的工具以及该工具的输入。\n",
    "  - 代理结束（AgentFinish）：表示代理程序已经完成，并包含向用户返回的信息。\n",
    "- 工具（Tools）：代理程序可以执行的操作。您向代理程序提供哪些工具高度取决于您希望代理程序执行的任务。\n",
    "- 工具包（Toolkits）：这些是为特定用例设计的工具组合。例如，为了使代理程序能够以最佳方式与SQL数据库交互，它可能需要访问一个工具来执行查询和另一个工具来检查表格。\n",
    "- 代理执行器（Agent Executor）：它包装了一个代理程序和一组工具。它负责循环地迭代运行代理程序，直到满足停止条件为止。\n",
    "\n",
    "\n",
    "\n",
    "LangChain Agent 是 LangChain 框架中的一个重要组成部分，它代表了智能合约的实例或具有特定功能的软件程序，旨在与现实世界进行交互并执行任务。以下是对 LangChain Agent 的详细解析：\n",
    "\n",
    "一、LangChain Agent 的基本概念\n",
    "\n",
    "LangChain 是一个开源的语言模型集成框架，旨在简化使用大型语言模型（LLM）创建应用程序的过程。Agent 在 LangChain 中扮演着核心角色，它利用语言模型（LLM）作为推理引擎，根据实时情况决定如何调用工具并执行任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.2 LangChain Agent 的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **接收任务**：用户给出一个任务（Prompt），这是 Agent 工作的起点。\n",
    "2. **思考（Thought）**：Agent 使用语言模型进行推理，制定解决问题的计划，并确定下一步需要采取的行动。\n",
    "3. **行动（Action）**：Agent 根据计划调用相应的工具（如搜索引擎、计算器、API等），并执行必要的操作。\n",
    "4. **观察（Observation）**：Agent 观察操作的结果，并将其作为新的上下文信息，用于后续的思考和行动。\n",
    "\n",
    "这个过程会循环进行，直到语言模型认为已经找到最终答案或达到预设的迭代次数。\n",
    "\n",
    "LangChain Agent 的类型\n",
    "\n",
    "LangChain 提供了多种类型的 Agent，以适应不同的应用场景和需求。主要包括以下几种类型：\n",
    "\n",
    "1. **动作代理人（Action Agents）**：在每个时间步上，使用所有先前动作的输出决定下一个动作。这类 Agent 适用于小任务或需要实时响应的场景。\n",
    "2. **计划执行代理人（Plan-and-execute Agents）**：预先决定所有动作的完整顺序，然后按照计划执行，而不更新计划。这类 Agent 适用于复杂或长时间运行的任务，需要保持长期目标和重点。\n",
    "\n",
    "LangChain Agent 的应用示例\n",
    "\n",
    "LangChain Agent 可以应用于各种任务，如文本生成、文档问答、聊天机器人、调用特定的 SaaS 服务等。以下是一个简单的应用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化 OpenAI 语言模型  \n",
    "llm = Tongyi()\n",
    "# 加载工具，例如数学计算工具\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "# 创建会话缓冲内存，用于保存对话历史\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "# 初始化 Agent，指定代理类型和工具\n",
    "agent = initialize_agent(\n",
    "    agent=\"conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory,\n",
    ")\n",
    "# 与 Agent 交互\n",
    "output_1 = agent.invoke(\"when you add 4 and 5 the result comes 10.\")\n",
    "output_2 = agent.invoke(\"4 + 5 is \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LangChain Agent 是一种强大的工具，它利用语言模型作为推理引擎，通过调用各种工具来执行复杂任务。不同类型的 Agent 适用于不同的应用场景，可以根据具体需求进行选择和配置。随着 LangChain 的不断发展和完善，Agent 的功能和性能也将不断提升，为开发者提供更加便捷和高效的解决方案。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.3 搭建工具\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\- serpai是一个聚合搜索引擎，需要安装谷歌搜索包以及申请账号 https://serpapi.com/manage-api-key\n",
    "\n",
    "\\- llm-math是一个封装好的数学计算链\n",
    "\n",
    "pip install google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "llm = Tongyi()\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,#这里有不同的类型\n",
    "    verbose=True,#是否打印日志\n",
    ")\n",
    "res = llm.invoke(\"请问2024年的美国总统是谁？他的年龄的除以2是多少?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.4 memory和agents配合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "# 定义tool\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 定义agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,#记忆组件\n",
    "    verbose=True,\n",
    ")\n",
    "print(agent.agent.llm_chain.prompt.template)\n",
    "agent.invoke(\"我是张三，今年18岁，性别女，现在在深圳工作，工作年限1年，月薪5000元\")\n",
    "print(agent.invoke(\"我的名字是什么?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "def buy_xlb(days: int):\n",
    "    return \"感冒发烧了\"\n",
    "def buy_jz(input: str):\n",
    "    #调用天气预报接口\n",
    "    # res = requests.get(\"?city=\"+input)\n",
    "    # data = json.loads(res.text)\n",
    "    return \"北京今天35度\"\n",
    "xlb = Tool.from_function(\n",
    "    func=buy_xlb,\n",
    "    name=\"buy_xlb\",\n",
    "    description=\"当用户咨询的是关于医疗问题的时候，使用这个工具，返回值为这个函数返回的结果\"\n",
    ")\n",
    "jz = Tool.from_function(\n",
    "    func=buy_jz,\n",
    "    name=\"buy_jz\",\n",
    "    description=\"当用户咨询的是关于天气问题的时候，使用这个工具，返回值为这个函数返回的结果\"\n",
    ")\n",
    "tools = [xlb,jz]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "res3 = agent.run(\"今天多少度\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2Agents 的类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- ZERO_SHOT_REACT_DESCRIPTION                   零样本反应描述，prompt没有用json格式化\n",
    "\n",
    "\\- CHAT_ZERO_SHOT_REACT_DESCRIPTION              聊天零样本反应描述，promp用json格式化\n",
    "\n",
    "\\- CONVERSATIONAL_REACT_DESCRIPTION              会话反应描述，需要memory\n",
    "\n",
    "\\- CHAT_CONVERSATIONAL_REACT_DESCRIPTION         聊天会话反应描述，需要memory\n",
    "\n",
    "\\- STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION   聊天结构化零样本反应描述，输出结果也是json格式化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1 ZERO_SHOT_REACT_DESCRIPTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "即在没有示例的情况下可以自主的进行对话的类型。\n",
    "\n",
    "应用场景：\n",
    "\n",
    "主要用于即时响应描述性任务，这些任务可能不需要复杂的对话上下文，而是直接针对某个问题或请求给出描述性回答。\n",
    "\n",
    "适用于那些需要快速生成准确描述的场景，如产品特性说明、数据报告解读等。\n",
    "\n",
    "交互方式：\n",
    "\n",
    "通常以单轮问答的形式进行，用户输入一个问题或请求，系统直接返回相应的描述性回答。\n",
    "\n",
    "不涉及复杂的对话历史或会话管理。\n",
    "\n",
    "生成内容特点：\n",
    "\n",
    "生成的回答侧重于直接、准确的描述，可能不包含过多的对话性语言或上下文依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "# 定义tools\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 定义agent--（tools、agent、llm、memory）\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "# print(agent.agent.llm_chain.prompt.messages[0].prompt.template)\n",
    "agent.invoke(\"现在美国总统是谁？他的年龄除以2是多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2CHAT_ZERO_SHOT_REACT_DESCRIPTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "零样本增强式生成，即在没有示例的情况下可以自主的进行对话的类型。\n",
    "\n",
    "应用场景：\n",
    "\n",
    "专为聊天和对话场景设计，能够处理多轮对话，并在对话过程中生成连贯、自然的回答。\n",
    "\n",
    "适用于需要与用户进行交互、理解用户意图并给出相应反应的场景，如聊天机器人、客服系统等。\n",
    "\n",
    "交互方式：\n",
    "\n",
    "支持多轮对话，能够处理用户的连续输入，并根据对话历史生成更加准确的回答。\n",
    "\n",
    "可能需要管理对话状态、跟踪用户意图和上下文信息。\n",
    "\n",
    "生成内容特点：\n",
    "\n",
    "生成的回答更加自然、流畅，能够适应用户的语言风格和对话习惯。\n",
    "\n",
    "可能包含对话性语言、问候语、确认信息等，以增强用户体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)\n",
    "# agent.invoke(\"现在美国总统是谁？他的年龄除以2是多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 CONVERSATIONAL_REACT_DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "一个对话型的agent，这个agent要求与memory一起使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "# 定义tool\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 定义agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,#记忆组件\n",
    "    verbose=True,\n",
    ")\n",
    "print(agent)\n",
    "print(agent.agent.llm_chain.prompt.template)\n",
    "agent.run(\"我是张三，今年18岁，性别女，现在在深圳工作，工作年限1年，月薪5000元\")\n",
    "agent.run(\"我的名字是什么?\")\n",
    "# agent.run(\"有什么好吃的泰国菜可以推荐给我吗?\")\n",
    "# agent.run(\"这些我都没吃过！我名字的最后一个字母是什么？1998年的世界杯谁夺冠了？\")\n",
    "# agent.run(\"中国陕西西安现在的气温多少？截止目前我们聊了什么？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4 CHAT_CONVERSATIONAL_REACT_DESCRIPTION 使用了chatmodel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "代理是 LangChain 框架中针对聊天和会话场景设计的一种高级代理类型。它利用 React 框架（在 LangChain 的上下文中，React 框架指的是一种用于决定何时调用哪个工具的逻辑框架，而非前端开发中的 React 库）来动态地选择和执行工具，同时利用会话记忆（如对话历史）来增强交互的连贯性和上下文理解能力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "# 定义tool\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    Tongyi(),\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,#记忆组件\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    # agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #agent类型 \n",
    "    # memory=memory,#记忆组件\n",
    "    # handle_parsing_errors=True,\n",
    "    verbose=True,\n",
    ")\n",
    "# print(\"1 ------------------------\")\n",
    "# print(len(agent.agent.llm_chain.prompt.messages))\n",
    "print(\"2 ------------------------\")\n",
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)\n",
    "# print(\"3 ------------------------\")\n",
    "# print(agent.agent.llm_chain.prompt.messages[1])\n",
    "# print(\"4 ------------------------\")\n",
    "# print(agent.agent.llm_chain.prompt.messages[2].prompt.template)\n",
    "# print(\"5 ------------------------\")\n",
    "# print(agent.agent.llm_chain.prompt.messages[3])\n",
    "# agent.run(\"有什么好吃的泰国菜可以推荐给我吗?用中文回答\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.5 STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#记忆组件\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "# 定义tool\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
    "# 定义agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    Tongyi(),\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #agent类型 \n",
    "    memory=memory,#记忆组件\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True,   \n",
    ")\n",
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)\n",
    "# print(agent.agent.llm_chain.prompt.messages[1].prompt.template)\n",
    "agent.run(\"有什么好吃的泰国菜可以推荐给我吗?用中文回答\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 内置的Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain预制了大量的tools，基本这些工具能满足大部分需求。 https://python.langchain.com.cn/docs/modules/agents/tools/\n",
    "\n",
    "\\- 加载预制tool的方法\n",
    "\n",
    "\\- 几种tool的使用方式\n",
    "\n",
    "对输出做了结构化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加预制工具的方法很简单\n",
    "from langchain.agents import load_tools\n",
    "tool_names = [...]\n",
    "tools = load_tools(tool_names) #使用load方法\n",
    "\n",
    "#有些tool需要单独设置llm\n",
    "from langchain.agents import load_tools\n",
    "tool_names = [...]\n",
    "llm = ...\n",
    "tools = load_tools(tool_names, llm=llm) #在load的时候指定llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1 SerpAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最常见的聚合搜索引擎 https://serper.dev/dashboard，支持google\\bing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities.serpapi import SerpAPIWrapper\n",
    "\n",
    "# 实例化\n",
    "search = SerpAPIWrapper()\n",
    "search.run(\"美国现在的总统是谁？\")\n",
    "# 支持自定义参数，比如将引擎切换到bing，设置搜索语言等\n",
    "params = {\n",
    "    \"engine\": \"bing\",\n",
    "    \"gl\": \"us\",\n",
    "    \"hl\": \"en\",\n",
    "}\n",
    "search = SerpAPIWrapper(params=params)\n",
    "search.run(\"美国现在的总统是谁？\")\n",
    "tools = load_tools([\"serpapi\"],llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2 Dall-E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall-E是openai出品的文到图AI大模型\n",
    "\n",
    "pip install opencv-python scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"dalle-image-generator\"])\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"Create an image of a halloween night at a haunted museum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.4 Eleven Labs Text2Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElevenLabs 是非常优秀的TTS合成API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ELEVEN_API_KEY\"] = \"23261e4a3b79697822252a505a169863\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'\n",
    "from langchain.tools.eleven_labs import ElevenLabsText2SpeechTool\n",
    "\n",
    "text_to_speak = \"Hello! 你好! Hola! नमस्ते! Bonjour! こんにちは! مرحبا! 안녕하세요! Ciao! Cześć! Привіт! வணக்கம்!\"\n",
    "tts = ElevenLabsText2SpeechTool(\n",
    "    voice=\"Bella\",\n",
    "    text_to_speak=text_to_speak,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file = tts.run(text_to_speak)\n",
    "speech_file = tts.run(text_to_speak)\n",
    "tts.stream_speech(text_to_speak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"eleven_labs_text2speech\"])\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"Create an image of a halloween night at a haunted museum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.5 GraphQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种api查询语言，类似sql，我们用它来查询奈飞的数据库，查找一下和星球大战相关的电影，API地址\n",
    "https://swapi-graphql.netlify.app/.netlify/functions/index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.utilities import GraphQLAPIWrapper\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"graphql\"],\n",
    "    graphql_endpoint=\"https://swapi-graphql.netlify.app/.netlify/functions/index\",\n",
    ")\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.6 Tookit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tookit是langchain已经封装好的一系列工具，一个工具包是一组工具来组合完成特定的任务\n",
    "\n",
    "一个python代码机器人\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    agent_executor_kwargs={\"handle_parsing_errors\": True},\n",
    ")\n",
    "agent_executor.run(\"生成10个斐波那契数列?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fibonacci_sequence(n):\n",
    "    sequence = []\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        sequence.append(a)\n",
    "        a, b = b, a + b\n",
    "    return sequence\n",
    "fibonacci_sequence = generate_fibonacci_sequence(10)\n",
    "fibonacci_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.7 SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "使用SQLDatabaseChain构建的agent，用来根据数据库回答一般行动饿问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///db/Chinook.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")\n",
    "# agent_executor.run(\"创建用户表，包括用户名和密码两个列\")\n",
    "agent_executor.run(\"\"\"用户表结构如下:\n",
    "                   CREATE TABLE users (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    username VARCHAR(50) NOT NULL UNIQUE,\n",
    "    password VARCHAR(50) NOT NULL\n",
    ");生成向用户表中插入3个数据的sql\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4自定义Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\- 定义一个class\n",
    "\n",
    "\\- 工具：默认搜索\n",
    "\n",
    "\\- 提示词：定义agent要做什么任务\n",
    "\n",
    "\\- outparse：约束LLM的行为和输出\n",
    "\n",
    "\\- 不同的LLM不同的质量\n",
    "\n",
    "自定义tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "import re\n",
    "import os\n",
    "\n",
    "class MyAgentTool:\n",
    "    def __init__(self) -> None:    \n",
    "        os.environ[\"SERPAPI_API_KEY\"] = 'db166b810c6b85674b6ceab3bd4e10d5048e1ba837db1c0d962ad91b34558805'    \n",
    "        self.serpapi = SerpAPIWrapper()\n",
    "    def tools(self):\n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"search\",\n",
    "                description=\"适用于当你需要回答关于当前事件的问题时\",\n",
    "                func=self.serpapi.run,\n",
    "            )\n",
    "        ]\n",
    "s = MyAgentTool()\n",
    "s.serpapi.run(\"python\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
